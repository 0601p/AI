{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8vNFy-h9oU"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56lfHfYoh9oV",
        "outputId": "b8baf30d-dee5-4eaa-c20b-cf055345e685"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"using\", device)\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3SMWmuFh9oW"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    # Hyperparameters\n",
        "    args = argparse.ArgumentParser(description='VAE')\n",
        "\n",
        "    # # Code for Module.py\n",
        "    # args.add_argument('--batch_size', type=int, default=256)\n",
        "    # args.add_argument('--epoch', type=int, default=500)\n",
        "    # args.add_argument('--lr', type=float, default=0.01)\n",
        "    # args.add_argument('--device', type=str, default=device)\n",
        "    # args.add_argument('--eval_per_epoch', type=int, default=10)\n",
        "    # args.parse_args()\n",
        "\n",
        "    # Code for Colab\n",
        "    args.batch_size = 2048\n",
        "    args.epoch = 500\n",
        "    args.lr = 0.0001\n",
        "    args.device = device\n",
        "    args.eval_per_epoch = 10\n",
        "    return args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLTraPl2h9oW"
      },
      "outputs": [],
      "source": [
        "class VAEEncoder(torch.nn.Module):\n",
        "    def __init__(self, size = (28, 28), latent_size = 32):\n",
        "        super().__init__()\n",
        "        self.size = 1\n",
        "        for i in size:\n",
        "            self.size *= i\n",
        "        self.latent_size = latent_size\n",
        "        self.fc1 = torch.nn.Linear(self.size, self.size//2)\n",
        "        self.fc2 = torch.nn.Linear(self.size//2, 256)\n",
        "        self.fc3_mu = torch.nn.Linear(256, latent_size)\n",
        "        self.fc3_var = torch.nn.Linear(256, latent_size)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : tensor size of(batch_size, size(at the init))\n",
        "        # q(z|x) ~ N(mu, var)\n",
        "        # mu : tensor size of (batch_size, latent_size(at the init))\n",
        "        # log_var : tensor size of (batch_size, latent_size(at the init)) -> this means log(sigma^2)\n",
        "        x = x.view(-1, self.size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        mu = self.fc3_mu(x)\n",
        "        log_var = self.fc3_var(x)\n",
        "        return mu, log_var\n",
        "\n",
        "\n",
        "class VAEDecoder(torch.nn.Module):\n",
        "    def __init__(self, size = (28, 28), latent_size = 32):\n",
        "        super().__init__()\n",
        "        self.orig_size = size\n",
        "        self.size = 1\n",
        "        for i in size:\n",
        "            self.size *= i\n",
        "        self.latent_size = latent_size\n",
        "        self.fc1 = torch.nn.Linear(self.latent_size, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, self.size//2)\n",
        "        self.fc3 = torch.nn.Linear(self.size//2, self.size)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : tensor size of(batch_size, latent_size(at the init))\n",
        "        # p(x|z) ~ Normal distribution\n",
        "        # return : tensor size of (batch_size, size(at the init))\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        x = x.view(-1, self.orig_size[0], self.orig_size[1])\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(self, size = (28, 28), latent_size = 32, device = 'cuda'):\n",
        "        # size : size of input (tuple)\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.latent_size = latent_size\n",
        "        self.device = device\n",
        "        self.encoder = VAEEncoder(size = self.size, latent_size = self.latent_size)\n",
        "        self.decoder = VAEDecoder(size = self.size, latent_size = self.latent_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        latent = self.sample(mu, log_var)\n",
        "        out = self.decoder(latent)\n",
        "        return mu, log_var, out\n",
        "\n",
        "    def sample(self, mu, log_var):\n",
        "        epsilon = torch.normal(0, 1, size=(mu.size(0), mu.size(1))).to(self.device)\n",
        "        return mu + epsilon * torch.exp(log_var / 2)\n",
        "\n",
        "    def make_latent(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        return self.sample(mu, log_var)\n",
        "\n",
        "    def forward_from_latent(self, latent):\n",
        "        out = self.decoder(latent)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpnEQrvdh9oX"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh8iMJcIh9oY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCivJqjqh9oY"
      },
      "outputs": [],
      "source": [
        "def VAELoss(ground_truth, mu_pred, logvar_pred, out_pred, reconst_func = torch.nn.MSELoss(reduction='sum')):\n",
        "    # p(x|z) ~ Normal distribution -> reconst func = torch.nn.MSELoss()\n",
        "    # p(x|z) ~ Bernoulli distribution -> reconst func = torch.nn.BCELoss()\n",
        "    if len(out_pred.size()) == 3:\n",
        "      out_pred = out_pred.unsqueeze(1)\n",
        "    Reconstruction_loss = reconst_func(ground_truth, out_pred) / out_pred.size(dim = 0)\n",
        "    KLD_loss = 0.5 * torch.mean(mu_pred.pow(2) + logvar_pred.exp() - logvar_pred - 1)\n",
        "    return Reconstruction_loss + KLD_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieHqGMWAh9oY"
      },
      "outputs": [],
      "source": [
        "def test(args, model, test_loader):\n",
        "    loss_avg = 0\n",
        "    total_img = 0\n",
        "    for img, _ in test_loader:\n",
        "        img = img.to(args.device)\n",
        "        mu_pred, logvar_pred, out_pred = model(img)\n",
        "        loss = VAELoss(img, mu_pred, logvar_pred, out_pred)\n",
        "\n",
        "        loss_avg += loss.item() * img.size(dim = 0)\n",
        "        total_img += img.size(dim = 0)\n",
        "    loss_avg /= total_img\n",
        "    print('[EVAL]\\t\\t loss :', loss_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEfy3aKYh9oY"
      },
      "outputs": [],
      "source": [
        "def train(args, model, train_loader, test_loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
        "    for i in range(args.epoch):\n",
        "        loss_avg = 0\n",
        "        total_img = 0\n",
        "        for img, _ in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            img = img.to(args.device)\n",
        "            mu_pred, logvar_pred, out_pred = model(img)\n",
        "            loss = VAELoss(img, mu_pred, logvar_pred, out_pred)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_avg += loss.item() * img.size(dim = 0)\n",
        "            total_img += img.size(dim = 0)\n",
        "\n",
        "        loss_avg /= total_img\n",
        "        print('[EPOCH]', (i + 1), '\\t\\t loss :', loss_avg)\n",
        "\n",
        "        if (i + 1) % args.eval_per_epoch == 0:\n",
        "            test(args, model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag7JvsLah9oZ"
      },
      "outputs": [],
      "source": [
        "def visualize(args, model, latent):\n",
        "    # latent : tensor size of (batch, latent_size)\n",
        "    # only first 10 latents will be visualized\n",
        "    latent = latent.to(args.device)\n",
        "    out = model.forward_from_latent(latent).cpu().detach().numpy()\n",
        "\n",
        "    size = latent.size(dim = 0)\n",
        "    size = size if size < 10 else 10\n",
        "    for i in range(size):\n",
        "        plt.subplot(size//2, 2, i + 1)\n",
        "        plt.imshow(out[i] * 256, cmap='gray')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQclSla5h9oZ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIJtGyUqh9oZ"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kp9Y3hoh9oZ"
      },
      "outputs": [],
      "source": [
        "args = parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRyTPkfKh9oZ"
      },
      "outputs": [],
      "source": [
        "mnist_train = torchvision.datasets.MNIST(\n",
        "    root = '../MNIST_data',\n",
        "    train = True,\n",
        "    transform = torchvision.transforms.ToTensor(),\n",
        "    download = True\n",
        ")\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    root = '../MNIST_data',\n",
        "    train = False,\n",
        "    transform = torchvision.transforms.ToTensor(),\n",
        "    download = True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdTFjP8ah9oa"
      },
      "outputs": [],
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    dataset = mnist_train,\n",
        "    shuffle = True,\n",
        "    batch_size = args.batch_size,\n",
        "    drop_last = True,\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    dataset = mnist_test,\n",
        "    shuffle = False,\n",
        "    batch_size = args.batch_size,\n",
        "    drop_last = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Kei0hIh9oa",
        "outputId": "231bed41-81ee-46d4-9d80-58f9fda12cac"
      },
      "outputs": [],
      "source": [
        "model = VAE(device = args.device).to(args.device)\n",
        "train(args, model, train_data_loader, test_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vGIeo5B5nb4Q",
        "outputId": "e6332b05-b04f-400d-bcc6-1e44953f279a"
      },
      "outputs": [],
      "source": [
        "# Inference from test data\n",
        "for img, _ in test_data_loader:\n",
        "    latent = model.make_latent(img[:10].to(args.device))\n",
        "    visualize(args, model, latent)\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DSOytywtKvMM",
        "outputId": "ec2fa805-b398-43f1-c781-00f558e15911"
      },
      "outputs": [],
      "source": [
        "# Inference from random noise\n",
        "visualize(args, model, torch.randn(10, 32))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
