{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image classification에 word embedding을 사용하면 어떻게 될까?\n",
    "\n",
    "-> CIFAR100 데이터의 레이블을 embedding된 vector로 바꾼 후에 그러한 vector를 예측하도록 regression 한 다음에, classification 한다.\n",
    "\n",
    "image -> embedding -> label 의 과정을 거쳐 classification 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"using\", device)\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vector_size = 50\n",
    "epoch = 500\n",
    "\n",
    "test_len = 10\n",
    "test_num = 50\n",
    "\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading glove\n",
    "embed_dict = {}\n",
    "with open(\"./glove6B/glove6B50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = numpy.asarray(values[1:], \"float32\")\n",
    "        embed_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels of cifar100, some words that are not in glove are replaced to None\n",
    "cifar_classnum_to_label=[\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"cra\", \"crocodile\", \"cup\", \"dinosaur\", \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"keyboard\", \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\", \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\", \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\", \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\", \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_classnum_to_embed = []\n",
    "for label in cifar_classnum_to_label:\n",
    "    if(label in embed_dict):\n",
    "        cifar_classnum_to_embed.append(embed_dict[label])\n",
    "    else:\n",
    "        words = label.split('_')\n",
    "        embed = embed_dict[words[0]] + embed_dict[words[1]]\n",
    "        cifar_classnum_to_embed.append(embed)\n",
    "\n",
    "cifar_classnum_to_embed = torch.FloatTensor(cifar_classnum_to_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR100(\n",
    "    root = '../CIFAR100_data',\n",
    "    train = True, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "\n",
    "cifar_test = torchvision.datasets.CIFAR100(\n",
    "    root = '../CIFAR100_data',\n",
    "    train = False, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(cifar_train, batch_size = batch_size, shuffle = True)\n",
    "data_loader_test = torch.utils.data.DataLoader(cifar_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_to_embed(torch.nn.Module):\n",
    "    def __init__(self, in_size = 3, channels = [16, 16, 32, 32, 64, 64, 128, 128], strides = [1, 1, 2, 1, 2, 1, 2, 1], half_size = False, linear_in = 128 * 4 * 4, linear_hidden = 128, linear_num = 2, linear_out = vector_size):\n",
    "        super(image_to_embed, self).__init__()\n",
    "        self.linear_in = linear_in\n",
    "        block_list = []\n",
    "\n",
    "        for i in range(len(channels)):\n",
    "            out_size = channels[i]\n",
    "            block_list.append(resblock(in_size, in_size, out_size, strides[i]))\n",
    "            in_size = out_size\n",
    "\n",
    "        self.blocks = torch.nn.Sequential(*block_list)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        linear_list = []\n",
    "        linear_list.append(torch.nn.Linear(linear_in, linear_hidden))\n",
    "        for i in range(linear_num):\n",
    "            linear_list.append(torch.nn.Linear(linear_hidden, linear_hidden))\n",
    "            linear_list.append(self.relu)\n",
    "        \n",
    "        linear_list.append(torch.nn.Linear(linear_hidden, linear_out))\n",
    "\n",
    "        self.linears = torch.nn.Sequential(*linear_list)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = out.view(-1, self.linear_in)\n",
    "        out = self.linears(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class resblock(torch.nn.Module): # in, hidden_channel은 말 그대로 channel 수, half_size는 output의 크기를 input 그대로 유지할 것인지, stride를 2로 할 것인지 결정\n",
    "    def __init__(self, in_channel, hidden_channel, out_channel, stride = 1):\n",
    "        super(resblock, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.hidden_channel = hidden_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.downsample = None\n",
    "\n",
    "        if(stride != 1 or in_channel != out_channel):\n",
    "            self.downsample = torch.nn.Conv2d(in_channel, out_channel, kernel_size=1, stride = stride)\n",
    "        self.bnskip = torch.nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "\n",
    "        self.conv1x1_in = torch.nn.Conv2d(in_channel, hidden_channel, kernel_size=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(hidden_channel)\n",
    "\n",
    "        self.conv3x3=torch.nn.Conv2d(hidden_channel, hidden_channel, kernel_size=3, padding=1, stride = stride)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(hidden_channel)\n",
    "\n",
    "        self.conv1x1_out = torch.nn.Conv2d(hidden_channel, out_channel, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1x1_in(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3x3(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv1x1_out(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if(self.downsample != None):\n",
    "            identity = self.downsample(x)\n",
    "        else:\n",
    "            identity = x\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = image_to_embed().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 LOSS : 0.3948608104835081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13696\\494291237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ParkMinSu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ParkMinSu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num = len(data_loader_train)\n",
    "for i in range(epoch):\n",
    "    avgloss = 0\n",
    "    for datax, datay in data_loader_train:\n",
    "        datax = datax.to(device)\n",
    "\n",
    "        datay = cifar_classnum_to_embed[datay]\n",
    "        predict = model(datax)\n",
    "\n",
    "        loss = criterion(predict, datay)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        avgloss += loss.item()\n",
    "\n",
    "    avgloss /= num\n",
    "    print(\"EPOCH\", i, \"LOSS :\", avgloss)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "020eb92898ac559bd98722b535e6235e035f297f752a3a61b26b02297b428980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
