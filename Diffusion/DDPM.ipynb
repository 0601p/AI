{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"using\", device)\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models.py\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class ContractingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return self.max_pool(x), x\n",
    "\n",
    "class ExpansiveBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.upconv = torch.nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size = 2, stride = 2)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, x_skip, time_embed):\n",
    "        x = self.upconv(x)\n",
    "        # batch_size and channel_input should be same size\n",
    "        assert x.size()[0] == x_skip.size()[0]\n",
    "        assert x.size()[1] == x_skip.size()[1]\n",
    "        if x.size() != x_skip.size():\n",
    "            # size conflict -> pad to align size\n",
    "            # this is only required if pad = 0 at Cont block and Exp block\n",
    "            assert x.size()[2] < x_skip.size()[2]\n",
    "            assert x.size()[3] < x_skip.size()[3]\n",
    "            x_dif = x_skip.size()[2] - x.size()[2]\n",
    "            y_dif = x_skip.size()[3] - x.size()[3]\n",
    "            # size will be aligned to x_skip\n",
    "            x = torch.nn.functional.pad(x, [x_dif // 2, x_dif - x_dif // 2, y_dif // 2, y_dif - y_dif // 2])\n",
    "        x = torch.cat((x, x_skip), dim = 1)\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class MiddleBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "    \n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # copy from transformer\n",
    "    def __init__(self, embed_len, steps) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(embed_len, steps)\n",
    "        encoding.requires_grad = False\n",
    "        position = torch.arange(0, embed_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, steps, 2) * -(math.log(10000.0) / steps))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = torch.nn.Parameter(data = encoding, requires_grad=False)\n",
    "    \n",
    "    # input size : \n",
    "    # (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        pos_embed = self.encoding[x, :]\n",
    "        return pos_embed\n",
    "\n",
    "class UnetForDiffusion(torch.nn.Module):\n",
    "    # Model of Unet with time embedding\n",
    "    # this differs from the model described in the DDPM paper\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, steps:int, embed_channels:int = 64, mid_start_channels:int = 64, path_len:int = 4):\n",
    "        # channel size inc/decrease like\n",
    "        # input image size should be larger than 2 ^ path_len\n",
    "        # in_channels -> mid_start_channels -> mid_start_channels * 2 ... -> mid_start_channels * 2 ^ path_len -> ... mid_start_channels -> out_channels\n",
    "        super().__init__()\n",
    "        cont_blocks = []\n",
    "        up_blocks = []\n",
    "        self.path_len = path_len\n",
    "        cont_blocks.append(ContractingBlock(in_channels, mid_start_channels, latent_size, embed_channels))\n",
    "        up_blocks.append(ExpansiveBlock(mid_start_channels * 2, mid_start_channels, latent_size, embed_channels))\n",
    "        self.time_encoding = PositionalEncoding(steps, embed_channels)\n",
    "        self.classifier = torch.nn.Conv2d(mid_start_channels, out_channels, kernel_size=1)\n",
    "        for _ in range(path_len - 1):\n",
    "            latent_size = [s // 2 for s in latent_size]\n",
    "            cont_blocks.append(ContractingBlock(mid_start_channels, mid_start_channels * 2, latent_size, embed_channels))\n",
    "            up_blocks.append(ExpansiveBlock(mid_start_channels * 4, mid_start_channels * 2, latent_size, embed_channels))\n",
    "            mid_start_channels *= 2\n",
    "        self.cont_blocks = torch.nn.ModuleList(cont_blocks)\n",
    "        self.exp_blocks = torch.nn.ModuleList(up_blocks)\n",
    "        latent_size = [s // 2 for s in latent_size]\n",
    "        self.mid_block = MiddleBlock(mid_start_channels, mid_start_channels * 2, latent_size, embed_channels)\n",
    "    \n",
    "\n",
    "    # input size : \n",
    "    # x : (batch_size, width, height)\n",
    "    # time : (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, width, height)\n",
    "    def forward(self, x, time):\n",
    "        skip = []\n",
    "        time_embed = self.time_encoding(time)\n",
    "        for i in range(self.path_len):\n",
    "            x, x_skip = self.cont_blocks[i](x, time_embed)\n",
    "            skip.append(x_skip)\n",
    "        x = self.mid_block(x, time_embed)\n",
    "        for i in range(self.path_len - 1, -1, -1):\n",
    "            x = self.exp_blocks[i](x, skip[i], time_embed)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class PixelCNNpp(torch.nn.Module):\n",
    "#     # Model of PixelCNN++\n",
    "#     def __init__(self, in_channels:int, out_channels:int):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking output size\n",
    "\n",
    "# net = Unet(in_channels = 3, out_channels = 10, steps = 1000)\n",
    "# inp = torch.randn(5, 3, 32, 32)\n",
    "# time = torch.arange(5)\n",
    "# out = net(inp, time)\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config.py\n",
    "\n",
    "class config:\n",
    "    # class to handle configs in ddpm\n",
    "    def __init__(self,\n",
    "                 in_channels = 1,\n",
    "                 out_channels = 1,\n",
    "                 latent_size = [28, 28],\n",
    "                 lr = 0.1, \n",
    "                 epoch = 1000, \n",
    "                 eval_per_epoch = 10, \n",
    "                 batch_size = 256,\n",
    "                 criterion = torch.nn.MSELoss(), \n",
    "                 step = 1000, \n",
    "                 beta_1 = 0.0001,\n",
    "                 beta_t = 0.02, \n",
    "                 device = device\n",
    "                 ):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.latent_size = latent_size\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.eval_per_epoch = eval_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.step = step\n",
    "        self.beta = [0] + [i / (step - 1) * (beta_t - beta_1) + beta_1 for i in range(0, step)]\n",
    "        assert len(self.beta) == step + 1\n",
    "\n",
    "        self.alpha = []\n",
    "        alpha = 1\n",
    "        for b_t in self.beta:\n",
    "            alpha *= (1 - b_t)\n",
    "            self.alpha.append(alpha)\n",
    "        self.device = device\n",
    "        self.beta = torch.tensor(self.beta).to(self.device)     # beta_t\n",
    "        self.alpha = torch.tensor(self.alpha).to(self.device)   # alpha_t bar\n",
    "        self.beta.requires_grad = False                         # Do not update alpha & beta\n",
    "        self.alpha.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM.py\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, model, train_data, eval_data, test_data, config:config):\n",
    "        # model should be image to image model has input size == output size\n",
    "        # data should be dataset, not dataloader\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "        # init dataloaders\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            dataset = train_data,\n",
    "            shuffle = True,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.eval_loader = torch.utils.data.DataLoader(\n",
    "            dataset = eval_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            dataset = test_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.config.lr)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(1, self.config.epoch + 1):\n",
    "            # train here\n",
    "            self.train_one_epoch(i)\n",
    "            \n",
    "            # eval here\n",
    "            if i % self.config.eval_per_epoch == 0:\n",
    "                self.evaluate(i)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        loss_sum = 0\n",
    "        cnt = 0\n",
    "        for x, _ in self.train_loader:\n",
    "            x = x.to(self.config.device)\n",
    "            sampled_steps = self.sample_steps()\n",
    "            x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "            x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "            pred_residual = self.model(x_tp1, sampled_steps)    # predict residual between x_t and x_t+1 using x_t+1\n",
    "            loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            cnt += x.size(0)\n",
    "        print('[EPOCH' + str(epoch) + '] TRAIN avg loss :', loss_sum / cnt)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        loss_sum = 0\n",
    "        cnt = 0\n",
    "        for x, _ in self.train_loader:\n",
    "            x = x.to(self.config.device)\n",
    "            sampled_steps = self.sample_steps()\n",
    "            x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "            x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "            pred_residual = self.model(x_tp1, sampled_steps)    # predict residual between x_t and x_t+1 using x_t+1\n",
    "            loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "            loss_sum += loss.item()\n",
    "            cnt += x.size(0)\n",
    "        print('[EPOCH' + str(epoch) + '] EVAL avg loss :', loss_sum / cnt)\n",
    "        return loss_sum / cnt\n",
    "\n",
    "    def inference(self, latent):\n",
    "        # inference from the latent\n",
    "        # latent : tensor of size (1, channels, width, height)\n",
    "        for step in range(self.config.step - 1, -1, -1):\n",
    "            latent = self.sample_reverse_1(latent, step)\n",
    "        \n",
    "        return latent\n",
    "\n",
    "    def sample_latent(self, size):\n",
    "        return torch.randn(size).to(self.config.device)\n",
    "    \n",
    "    def sample_reverse_1(self, x_t, step):\n",
    "        step_torch = torch.tensor([step])\n",
    "        z = torch.randn(x_t.size()).to(self.config.device)\n",
    "        sigma_t = torch.sqrt(self.config.beta[step_torch])      # page 3 of paper says that (1 - alpha_t-1) / (1 - alpha_t) * beta_t and beta_t had similar results\n",
    "        return (x_t - self.model(x_t, step_torch) * ((self.config.beta[step_torch]) / torch.sqrt(1 - self.config.alpha[step_torch]))[:, None, None, None]) / torch.sqrt(1 - self.config.beta[step_torch])[:, None, None, None] + z * sigma_t[:, None, None, None]\n",
    "    \n",
    "    def sample_steps(self):\n",
    "        return torch.randint(self.config.step, (self.config.batch_size, ))\n",
    "    \n",
    "    def sample_forward_t(self, x_0, step):\n",
    "        # sample using q(x_t|x_0)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t\n",
    "        return torch.randn(x_0.size()).to(self.config.device) * (1 - self.config.alpha[step])[:, None, None, None] + x_0 * torch.sqrt(self.config.alpha[step])[:, None, None, None]\n",
    "\n",
    "    def sample_forward_1(self, x_t, step):\n",
    "        # sample using q(x_t+1|x_t)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t+1\n",
    "        return torch.randn(x_t.size()).to(self.config.device) * (self.config.beta[step + 1])[:, None, None, None] + x_t * torch.sqrt(1 - self.config.beta[step + 1])[:, None, None, None]\n",
    "\n",
    "    def loss(self, pred_x_t, gt_x_t, gt_x_tp1, step):\n",
    "        # mseloss\n",
    "        # input : predicted x_t, gt x_t, gt x_t+1, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : mseloss with prediction and gt, \n",
    "        gt_residual = (gt_x_t - gt_x_tp1) / self.config.beta[step + 1][:, None, None, None]\n",
    "        return torch.sum(torch.mean((pred_x_t - gt_residual).pow(2), 0, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPMConfig = config()\n",
    "model = UnetForDiffusion(in_channels = DDPMConfig.in_channels, out_channels = DDPMConfig.out_channels, latent_size = DDPMConfig.latent_size, steps = 1000).to(DDPMConfig.device)\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = True, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "mnist_eval = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = False, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "DDPM_obj = DDPM(model, mnist_train, mnist_eval, mnist_eval, DDPMConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH1] TRAIN avg loss : 3.6297121047973633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DDPM_obj\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[209], line 34\u001b[0m, in \u001b[0;36mDDPM.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39m# train here\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(i)\n\u001b[1;32m     36\u001b[0m         \u001b[39m# eval here\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meval_per_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[209], line 49\u001b[0m, in \u001b[0;36mDDPM.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m x_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_forward_t(x, sampled_steps)         \u001b[39m# sampled x_t\u001b[39;00m\n\u001b[1;32m     48\u001b[0m x_tp1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_forward_1(x_t, sampled_steps)     \u001b[39m# sampled x_t+1\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m pred_residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x_tp1, sampled_steps)    \u001b[39m# predict residual between x_t and x_t+1 using x_t+1\u001b[39;00m\n\u001b[1;32m     50\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(pred_residual, x_t, x_tp1, sampled_steps)\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[205], line 130\u001b[0m, in \u001b[0;36mUnetForDiffusion.forward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m    128\u001b[0m     x, x_skip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_blocks[i](x, time_embed)\n\u001b[1;32m    129\u001b[0m     skip\u001b[39m.\u001b[39mappend(x_skip)\n\u001b[0;32m--> 130\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmid_block(x, time_embed)\n\u001b[1;32m    131\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_len \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    132\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexp_blocks[i](x, skip[i], time_embed)\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[205], line 67\u001b[0m, in \u001b[0;36mMiddleBlock.forward\u001b[0;34m(self, x, time_embed)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, time_embed):\n\u001b[0;32m---> 67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm1(x)) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embed_fc(time_embed)[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m     69\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm2(x))\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DDPM_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_latent = DDPM_obj.sample_latent((1, 1, 32, 32))\n",
    "inferenced_image = DDPM_obj.inference(example_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "def print_image(img_torch):\n",
    "    img = img_torch.numpy()\n",
    "    plt.imshow(numpy.transpose(img, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(inferenced_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
