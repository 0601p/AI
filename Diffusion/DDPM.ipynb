{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"using\", device)\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models.py\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class ContractingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(in_channels, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(out_channels, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return self.max_pool(x), x\n",
    "\n",
    "class ExpansiveBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.upconv = torch.nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size = 2, stride = 2)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(in_channels, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(out_channels, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, x_skip, time_embed):\n",
    "        x = self.upconv(x)\n",
    "        # batch_size and channel_input should be same size\n",
    "        assert x.size()[0] == x_skip.size()[0]\n",
    "        assert x.size()[1] == x_skip.size()[1]\n",
    "        if x.size() != x_skip.size():\n",
    "            # size conflict -> pad to align size\n",
    "            # this is only required if pad = 0 at Cont block and Exp block\n",
    "            assert x.size()[2] < x_skip.size()[2]\n",
    "            assert x.size()[3] < x_skip.size()[3]\n",
    "            x_dif = x_skip.size()[2] - x.size()[2]\n",
    "            y_dif = x_skip.size()[3] - x.size()[3]\n",
    "            # size will be aligned to x_skip\n",
    "            x = torch.nn.functional.pad(x, [x_dif // 2, x_dif - x_dif // 2, y_dif // 2, y_dif - y_dif // 2])\n",
    "        x = torch.cat((x, x_skip), dim = 1)\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class MiddleBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(in_channels, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(out_channels, eps = 1e-5)\n",
    "    \n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # copy from transformer\n",
    "    def __init__(self, embed_len, steps) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(embed_len, steps)\n",
    "        encoding.requires_grad = False\n",
    "        position = torch.arange(0, embed_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, steps, 2) * -(math.log(10000.0) / steps))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = torch.nn.Parameter(data = encoding, requires_grad=False)\n",
    "    \n",
    "    # input size : \n",
    "    # (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        pos_embed = self.encoding[x, :]\n",
    "        return pos_embed\n",
    "\n",
    "class UnetForDiffusion(torch.nn.Module):\n",
    "    # Model of Unet with time embedding\n",
    "    # this differs from the model described in the DDPM paper\n",
    "    def __init__(self, in_channels:int, out_channels:int, steps:int, embed_channels:int = 64, mid_start_channels:int = 64, path_len:int = 4):\n",
    "        # channel size inc/decrease like\n",
    "        # input image size should be larger than 2 ^ path_len\n",
    "        # in_channels -> mid_start_channels -> mid_start_channels * 2 ... -> mid_start_channels * 2 ^ path_len -> ... mid_start_channels -> out_channels\n",
    "        super().__init__()\n",
    "        cont_blocks = []\n",
    "        up_blocks = []\n",
    "        self.path_len = path_len\n",
    "        cont_blocks.append(ContractingBlock(in_channels, mid_start_channels, embed_channels))\n",
    "        up_blocks.append(ExpansiveBlock(mid_start_channels * 2, mid_start_channels, embed_channels))\n",
    "        self.time_encoding = PositionalEncoding(steps, embed_channels)\n",
    "        self.classifier = torch.nn.Conv2d(mid_start_channels, out_channels, kernel_size=1)\n",
    "        for _ in range(path_len - 1):\n",
    "            cont_blocks.append(ContractingBlock(mid_start_channels, mid_start_channels * 2, embed_channels))\n",
    "            up_blocks.append(ExpansiveBlock(mid_start_channels * 4, mid_start_channels * 2, embed_channels))\n",
    "            mid_start_channels *= 2\n",
    "        self.cont_blocks = torch.nn.ModuleList(cont_blocks)\n",
    "        self.exp_blocks = torch.nn.ModuleList(up_blocks)\n",
    "        self.mid_block = MiddleBlock(mid_start_channels, mid_start_channels * 2, embed_channels)\n",
    "    \n",
    "\n",
    "    # input size : \n",
    "    # x : (batch_size, width, height)\n",
    "    # time : (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, width, height)\n",
    "    def forward(self, x, time):\n",
    "        skip = []\n",
    "        time_embed = self.time_encoding(time)\n",
    "        for i in range(self.path_len):\n",
    "            x, x_skip = self.cont_blocks[i](x, time_embed)\n",
    "            skip.append(x_skip)\n",
    "        x = self.mid_block(x, time_embed)\n",
    "        for i in range(self.path_len - 1, -1, -1):\n",
    "            x = self.exp_blocks[i](x, skip[i], time_embed)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class PixelCNNpp(torch.nn.Module):\n",
    "#     # Model of PixelCNN++\n",
    "#     def __init__(self, in_channels:int, out_channels:int):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking output size\n",
    "\n",
    "# net = Unet(in_channels = 3, out_channels = 10, steps = 1000)\n",
    "# inp = torch.randn(5, 3, 32, 32)\n",
    "# time = torch.arange(5)\n",
    "# out = net(inp, time)\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config.py\n",
    "\n",
    "class config:\n",
    "    # class to handle configs in ddpm\n",
    "    def __init__(self, lr = 0.1, \n",
    "                 epoch = 1000, \n",
    "                 eval_per_epoch = 10, \n",
    "                 batch_size = 128,\n",
    "                 criterion = torch.nn.MSELoss(), \n",
    "                 step = 1000, \n",
    "                 beta_1 = 0.0001,\n",
    "                 beta_t = 0.02, \n",
    "                 device = device\n",
    "                 ):\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.eval_per_epoch = eval_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.step = step\n",
    "        self.beta = [0] + [i / (step - 1) * (beta_t - beta_1) + beta_1 for i in range(0, step)]\n",
    "        assert len(self.beta) == step + 1\n",
    "\n",
    "        self.alpha = []\n",
    "        alpha = 1\n",
    "        for b_t in self.beta:\n",
    "            alpha *= (1 - b_t)\n",
    "            self.alpha.append(alpha)\n",
    "        self.device = device\n",
    "        self.beta = torch.tensor(self.beta)     # beta_t\n",
    "        self.alpha = torch.tensor(self.alpha)   # alpha_t bar\n",
    "        self.beta.requires_grad = False         # Do not update alpha & beta\n",
    "        self.alpha.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM.py\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, model, train_data, eval_data, test_data, config:config):\n",
    "        # model should be image to image model has input size == output size\n",
    "        # data should be dataset, not dataloader\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "        # init dataloaders\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            dataset = train_data,\n",
    "            shuffle = True,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.eval_loader = torch.utils.data.DataLoader(\n",
    "            dataset = eval_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            dataset = test_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.config.lr)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(1, self.config.epoch + 1):\n",
    "            # train here\n",
    "            self.train_one_epoch(i)\n",
    "            \n",
    "            # eval here\n",
    "            if i % self.config.eval_per_epoch == 0:\n",
    "                self.evaluate(i)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        loss_sum = 0\n",
    "        cnt = 0\n",
    "        for x, _ in self.train_loader:\n",
    "            x = x.to(self.config.device)\n",
    "            sampled_steps = self.sample_steps()\n",
    "            x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "            x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "            pred_residual = self.model(x_tp1, sampled_steps)    # predict residual between x_t and x_t+1 using x_t+1\n",
    "            loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            cnt += x.size(0)\n",
    "        print('[EPOCH' + str(epoch) + '] TRAIN avg loss :', loss_sum / cnt)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        loss_sum = 0\n",
    "        cnt = 0\n",
    "        for x, _ in self.train_loader:\n",
    "            x = x.to(self.config.device)\n",
    "            sampled_steps = self.sample_steps()\n",
    "            x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "            x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "            pred_residual = self.model(x_tp1, sampled_steps)    # predict residual between x_t and x_t+1 using x_t+1\n",
    "            loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "            loss_sum += loss.item()\n",
    "            cnt += x.size(0)\n",
    "        print('[EPOCH' + str(epoch) + '] EVAL avg loss :', loss_sum / cnt)\n",
    "        return loss_sum / cnt\n",
    "\n",
    "    def inference(self, latent):\n",
    "        # inference from the latent\n",
    "        # latent : tensor of size (1, channels, width, height)\n",
    "        for step in range(self.config.step - 1, -1, -1):\n",
    "            latent = self.sample_reverse_1(latent, step)\n",
    "        \n",
    "        return latent\n",
    "\n",
    "    def sample_latent(self, size):\n",
    "        return torch.randn(size).to(self.config.device)\n",
    "    \n",
    "    def sample_reverse_1(self, x_t, step):\n",
    "        step_torch = torch.tensor([step])\n",
    "        z = torch.randn(x_t.size()).to(self.config.device)\n",
    "        sigma_t = torch.sqrt(self.config.beta[step_torch])      # page 3 of paper says that (1 - alpha_t-1) / (1 - alpha_t) * beta_t and beta_t had similar results\n",
    "        return (x_t - self.model(x_t, step_torch) * ((self.config.beta[step_torch]) / torch.sqrt(1 - self.config.alpha[step_torch]))[:, None, None, None]) / torch.sqrt(1 - self.config.beta[step_torch])[:, None, None, None] + z * sigma_t[:, None, None, None]\n",
    "    \n",
    "    def sample_steps(self):\n",
    "        return torch.randint(self.config.step, (self.config.batch_size, ))\n",
    "    \n",
    "    def sample_forward_t(self, x_0, step):\n",
    "        # sample using q(x_t|x_0)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t\n",
    "        return torch.randn(x_0.size()).to(self.config.device) * (1 - self.config.alpha[step])[:, None, None, None] + x_0 * torch.sqrt(self.config.alpha[step])[:, None, None, None]\n",
    "\n",
    "    def sample_forward_1(self, x_t, step):\n",
    "        # sample using q(x_t+1|x_t)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t+1\n",
    "        return torch.randn(x_t.size()).to(self.config.device) * (self.config.beta[step + 1])[:, None, None, None] + x_t * torch.sqrt(1 - self.config.beta[step + 1])[:, None, None, None]\n",
    "\n",
    "    def loss(self, pred_x_t, gt_x_t, gt_x_tp1, step):\n",
    "        # mseloss\n",
    "        # input : predicted x_t, gt x_t, gt x_t+1, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : mseloss with prediction and gt, \n",
    "        gt_residual = (gt_x_t - gt_x_tp1) / self.config.beta[step + 1][:, None, None, None]\n",
    "        return torch.sum(torch.mean((pred_x_t - gt_residual).pow(2), 0, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPMConfig = config()\n",
    "model = UnetForDiffusion(in_channels = 1, out_channels = 1, steps = 1000).to(DDPMConfig.device)\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = True, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "mnist_eval = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = False, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "DDPM_obj = DDPM(model, mnist_train, mnist_eval, mnist_eval, DDPMConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPM_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_latent = DDPM_obj.sample_latent((1, 1, 32, 32))\n",
    "inferenced_image = DDPM_obj.inference(example_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "def print_image(img_torch):\n",
    "    img = img_torch.numpy()\n",
    "    plt.imshow(numpy.transpose(img, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(inferenced_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
