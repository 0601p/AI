{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"using\", device)\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models.py\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class ContractingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return self.max_pool(x), x\n",
    "\n",
    "class ExpansiveBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.upconv = torch.nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size = 2, stride = 2)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "\n",
    "    def forward(self, x, x_skip, time_embed):\n",
    "        x = self.upconv(x)\n",
    "        # batch_size and channel_input should be same size\n",
    "        assert x.size()[0] == x_skip.size()[0]\n",
    "        assert x.size()[1] == x_skip.size()[1]\n",
    "        if x.size() != x_skip.size():\n",
    "            # size conflict -> pad to align size\n",
    "            # this is only required if pad = 0 at Cont block and Exp block\n",
    "            assert x.size()[2] < x_skip.size()[2]\n",
    "            assert x.size()[3] < x_skip.size()[3]\n",
    "            x_dif = x_skip.size()[2] - x.size()[2]\n",
    "            y_dif = x_skip.size()[3] - x.size()[3]\n",
    "            # size will be aligned to x_skip\n",
    "            x = torch.nn.functional.pad(x, [x_dif // 2, x_dif - x_dif // 2, y_dif // 2, y_dif - y_dif // 2])\n",
    "        x = torch.cat((x, x_skip), dim = 1)\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class MiddleBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, embed_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        self.time_embed_fc = torch.nn.Linear(embed_channels, out_channels)\n",
    "        self.layernorm1 = torch.nn.LayerNorm([in_channels] + latent_size, eps = 1e-5)\n",
    "        self.layernorm2 = torch.nn.LayerNorm([out_channels] + latent_size, eps = 1e-5)\n",
    "    \n",
    "    def forward(self, x, time_embed):\n",
    "        x = self.conv1(self.layernorm1(x)) + self.time_embed_fc(time_embed)[:, :, None, None]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(self.layernorm2(x))\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # copy from transformer\n",
    "    def __init__(self, embed_len, steps) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(embed_len, steps)\n",
    "        encoding.requires_grad = False\n",
    "        position = torch.arange(0, embed_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, steps, 2) * -(math.log(10000.0) / steps))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = torch.nn.Parameter(data = encoding, requires_grad=False)\n",
    "    \n",
    "    # input size : \n",
    "    # (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        pos_embed = self.encoding[x, :]\n",
    "        return pos_embed\n",
    "\n",
    "class UnetForDiffusion(torch.nn.Module):\n",
    "    # Model of Unet with time embedding\n",
    "    # this differs from the model described in the DDPM paper\n",
    "    def __init__(self, in_channels:int, out_channels:int, latent_size:list, steps:int, embed_channels:int = 64, mid_start_channels:int = 64, path_len:int = 4):\n",
    "        # channel size inc/decrease like\n",
    "        # input image size should be larger than 2 ^ path_len\n",
    "        # in_channels -> mid_start_channels -> mid_start_channels * 2 ... -> mid_start_channels * 2 ^ path_len -> ... mid_start_channels -> out_channels\n",
    "        super().__init__()\n",
    "        cont_blocks = []\n",
    "        up_blocks = []\n",
    "        self.path_len = path_len\n",
    "        cont_blocks.append(ContractingBlock(in_channels, mid_start_channels, latent_size, embed_channels))\n",
    "        up_blocks.append(ExpansiveBlock(mid_start_channels * 2, mid_start_channels, latent_size, embed_channels))\n",
    "        self.time_encoding = PositionalEncoding(steps, embed_channels)\n",
    "        self.classifier = torch.nn.Conv2d(mid_start_channels, out_channels, kernel_size=1)\n",
    "        for _ in range(path_len - 1):\n",
    "            latent_size = [s // 2 for s in latent_size]\n",
    "            cont_blocks.append(ContractingBlock(mid_start_channels, mid_start_channels * 2, latent_size, embed_channels))\n",
    "            up_blocks.append(ExpansiveBlock(mid_start_channels * 4, mid_start_channels * 2, latent_size, embed_channels))\n",
    "            mid_start_channels *= 2\n",
    "        self.cont_blocks = torch.nn.ModuleList(cont_blocks)\n",
    "        self.exp_blocks = torch.nn.ModuleList(up_blocks)\n",
    "        latent_size = [s // 2 for s in latent_size]\n",
    "        self.mid_block = MiddleBlock(mid_start_channels, mid_start_channels * 2, latent_size, embed_channels)\n",
    "    \n",
    "\n",
    "    # input size : \n",
    "    # x : (batch_size, width, height)\n",
    "    # time : (batch_size) << this should be integer(time)\n",
    "    # output size :\n",
    "    # (batch_size, width, height)\n",
    "    def forward(self, x, time):\n",
    "        skip = []\n",
    "        time_embed = self.time_encoding(time)\n",
    "        for i in range(self.path_len):\n",
    "            x, x_skip = self.cont_blocks[i](x, time_embed)\n",
    "            skip.append(x_skip)\n",
    "        x = self.mid_block(x, time_embed)\n",
    "        for i in range(self.path_len - 1, -1, -1):\n",
    "            x = self.exp_blocks[i](x, skip[i], time_embed)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class PixelCNNpp(torch.nn.Module):\n",
    "#     # Model of PixelCNN++\n",
    "#     def __init__(self, in_channels:int, out_channels:int):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking output size\n",
    "\n",
    "# net = Unet(in_channels = 3, out_channels = 10, steps = 1000)\n",
    "# inp = torch.randn(5, 3, 32, 32)\n",
    "# time = torch.arange(5)\n",
    "# out = net(inp, time)\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config.py\n",
    "\n",
    "class config:\n",
    "    # class to handle configs in ddpm\n",
    "    def __init__(self,\n",
    "                 in_channels = 1,\n",
    "                 out_channels = 1,\n",
    "                 latent_size = [28, 28],\n",
    "                 lr = 0.001, \n",
    "                 epoch = 1000, \n",
    "                 eval_per_epoch = 10, \n",
    "                 batch_size = 1024,\n",
    "                 criterion = torch.nn.MSELoss(), \n",
    "                 step = 1000, \n",
    "                 beta_1 = 0.0001,\n",
    "                 beta_t = 0.02, \n",
    "                 device = device\n",
    "                 ):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.latent_size = latent_size\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.eval_per_epoch = eval_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.step = step\n",
    "        self.beta = [0] + [i / (step - 1) * (beta_t - beta_1) + beta_1 for i in range(0, step)]\n",
    "        assert len(self.beta) == step + 1\n",
    "\n",
    "        self.alpha = []\n",
    "        alpha = 1\n",
    "        for b_t in self.beta:\n",
    "            alpha *= (1 - b_t)\n",
    "            self.alpha.append(alpha)\n",
    "        self.device = device\n",
    "        self.beta = torch.tensor(self.beta).to(self.device)     # beta_t\n",
    "        self.alpha = torch.tensor(self.alpha).to(self.device)   # alpha_t bar\n",
    "        self.beta.requires_grad = False                         # Do not update alpha & beta\n",
    "        self.alpha.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM.py\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, model, train_data, eval_data, test_data, config:config):\n",
    "        # model should be image to image model has input size == output size\n",
    "        # data should be dataset, not dataloader\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "        # init dataloaders\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            dataset = train_data,\n",
    "            shuffle = True,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.eval_loader = torch.utils.data.DataLoader(\n",
    "            dataset = eval_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            dataset = test_data,\n",
    "            shuffle = False,\n",
    "            batch_size = self.config.batch_size,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.config.lr)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(1, self.config.epoch + 1):\n",
    "            # train here\n",
    "            self.train_one_epoch(i)\n",
    "            \n",
    "            # eval here\n",
    "            if i % self.config.eval_per_epoch == 0:\n",
    "                self.evaluate(i)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        loss_sum = 0\n",
    "        cnt = 0\n",
    "        for x, _ in self.train_loader:\n",
    "            x = x.to(self.config.device)\n",
    "            sampled_steps = self.sample_steps()\n",
    "            x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "            x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "            pred_residual = self.model(x_tp1, sampled_steps)    # predict residual between x_t and x_t+1 using x_t+1\n",
    "            loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            cnt += x.size(0)\n",
    "        print('[EPOCH' + str(epoch) + '] TRAIN avg loss :', loss_sum / cnt)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            cnt = 0\n",
    "            for x, _ in self.train_loader:\n",
    "                x = x.to(self.config.device)\n",
    "                sampled_steps = self.sample_steps()\n",
    "                x_t = self.sample_forward_t(x, sampled_steps)         # sampled x_t\n",
    "                x_tp1 = self.sample_forward_1(x_t, sampled_steps)     # sampled x_t+1\n",
    "                pred_residual = self.model(x_tp1, sampled_steps)      # predict residual between x_t and x_t+1 using x_t+1\n",
    "                loss = self.loss(pred_residual, x_t, x_tp1, sampled_steps)\n",
    "                loss_sum += loss.item()\n",
    "                cnt += x.size(0)\n",
    "            print('[EPOCH' + str(epoch) + '] EVAL avg loss :', loss_sum / cnt)\n",
    "            return loss_sum / cnt\n",
    "\n",
    "    def inference(self, latent):\n",
    "        # inference from the latent\n",
    "        # latent : tensor of size (1, channels, width, height)\n",
    "        for step in range(self.config.step - 1, 0, -1):\n",
    "            latent = self.sample_reverse_1(latent, step)\n",
    "        \n",
    "        return latent\n",
    "\n",
    "    def sample_latent(self, size):\n",
    "        return torch.randn(size).to(self.config.device)\n",
    "    \n",
    "    def sample_reverse_1(self, x_t, step):\n",
    "        step_torch = torch.tensor([step])\n",
    "        z = torch.randn(x_t.size()).to(self.config.device)\n",
    "        sigma_t = torch.sqrt(self.config.beta[step_torch])      # page 3 of paper says that (1 - alpha_t-1) / (1 - alpha_t) * beta_t and beta_t had similar results\n",
    "        return (x_t - self.model(x_t, step_torch) * ((self.config.beta[step_torch]) / torch.sqrt(1 - self.config.alpha[step_torch]))[:, None, None, None]) / torch.sqrt(1 - self.config.beta[step_torch])[:, None, None, None] + z * sigma_t[:, None, None, None]\n",
    "    \n",
    "    def sample_steps(self):\n",
    "        return torch.randint(low = 1, high = self.config.step + 1, size = (self.config.batch_size, ))\n",
    "    \n",
    "    def sample_forward_t(self, x_0, step):\n",
    "        # sample using q(x_t|x_0)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t\n",
    "        return torch.randn(x_0.size()).to(self.config.device) * (1 - self.config.alpha[step])[:, None, None, None] + x_0 * torch.sqrt(self.config.alpha[step])[:, None, None, None]\n",
    "\n",
    "    def sample_forward_1(self, x_t, step):\n",
    "        # sample using q(x_t+1|x_t)\n",
    "        # input : x_0, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : sampled x_t+1\n",
    "        return torch.randn(x_t.size()).to(self.config.device) * (self.config.beta[step + 1])[:, None, None, None] + x_t * torch.sqrt(1 - self.config.beta[step + 1])[:, None, None, None]\n",
    "\n",
    "    def loss(self, pred_x_t, gt_x_t, gt_x_tp1, step):\n",
    "        # mseloss\n",
    "        # input : predicted x_t, gt x_t, gt x_t+1, step(t)\n",
    "        # step must be 0 ~ max_step - 1\n",
    "        # output : mseloss with prediction and gt, \n",
    "        gt_residual = (gt_x_t - gt_x_tp1) / self.config.beta[step + 1][:, None, None, None]\n",
    "        return torch.sum(torch.mean((pred_x_t - gt_residual).pow(2), 0, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPMConfig = config()\n",
    "model = UnetForDiffusion(in_channels = DDPMConfig.in_channels, out_channels = DDPMConfig.out_channels, latent_size = DDPMConfig.latent_size, steps = 1000).to(DDPMConfig.device)\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = True, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "mnist_eval = torchvision.datasets.MNIST(\n",
    "    root = '../MNIST_data',\n",
    "    train = False, \n",
    "    transform = torchvision.transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "DDPM_obj = DDPM(model, mnist_train, mnist_eval, mnist_eval, DDPMConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DDPM_obj\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[233], line 34\u001b[0m, in \u001b[0;36mDDPM.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39m# train here\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(i)\n\u001b[1;32m     36\u001b[0m         \u001b[39m# eval here\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meval_per_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[233], line 49\u001b[0m, in \u001b[0;36mDDPM.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m x_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_forward_t(x, sampled_steps)         \u001b[39m# sampled x_t\u001b[39;00m\n\u001b[1;32m     48\u001b[0m x_tp1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_forward_1(x_t, sampled_steps)     \u001b[39m# sampled x_t+1\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m pred_residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x_tp1, sampled_steps)    \u001b[39m# predict residual between x_t and x_t+1 using x_t+1\u001b[39;00m\n\u001b[1;32m     50\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(pred_residual, x_t, x_tp1, sampled_steps)\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[229], line 130\u001b[0m, in \u001b[0;36mUnetForDiffusion.forward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m    128\u001b[0m     x, x_skip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_blocks[i](x, time_embed)\n\u001b[1;32m    129\u001b[0m     skip\u001b[39m.\u001b[39mappend(x_skip)\n\u001b[0;32m--> 130\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmid_block(x, time_embed)\n\u001b[1;32m    131\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_len \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    132\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexp_blocks[i](x, skip[i], time_embed)\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[229], line 69\u001b[0m, in \u001b[0;36mMiddleBlock.forward\u001b[0;34m(self, x, time_embed)\u001b[0m\n\u001b[1;32m     67\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm1(x)) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embed_fc(time_embed)[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     68\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[0;32m---> 69\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm2(x))\n\u001b[1;32m     70\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda/envs/common/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DDPM_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.0970e-01, -4.7024e-02, -8.2128e-01, -1.5742e+00, -7.3629e-01,\n",
      "            2.3007e+00, -1.1734e+00, -6.0853e-01, -6.2319e-01,  7.1948e-02,\n",
      "           -2.6656e-01, -1.8607e-01, -7.4003e-01,  9.5177e-01, -1.0995e+00,\n",
      "           -1.2623e+00, -5.7728e-01,  4.5691e-01, -5.5453e-01,  1.0887e-01,\n",
      "           -9.7734e-01,  2.6716e-01,  1.5384e+00,  8.6611e-01,  6.2772e-01,\n",
      "            2.9728e-01,  1.3412e+00,  5.4301e-01],\n",
      "          [ 1.0315e+00,  1.1674e+00,  1.1869e+00, -6.8598e-01,  1.4556e+00,\n",
      "           -9.5803e-01, -7.8348e-01, -1.2823e+00,  5.4614e-01,  5.0915e-01,\n",
      "            5.0229e-01,  4.9673e-01, -1.9772e+00, -8.3901e-01,  5.3505e-01,\n",
      "            1.0929e+00,  2.3293e-01,  1.0791e+00, -1.5583e+00, -1.9998e+00,\n",
      "           -7.8043e-01, -1.2233e+00,  2.4887e+00,  2.1653e-02,  1.2196e+00,\n",
      "            2.4951e-01,  1.3177e-01, -2.2561e-01],\n",
      "          [-3.4554e-01,  1.0577e+00, -7.5481e-01,  1.5030e+00,  3.3363e-01,\n",
      "           -7.0416e-01,  1.0160e+00, -1.2566e-02, -1.7288e+00, -5.4981e-01,\n",
      "            1.4144e+00,  9.8463e-01,  4.4597e-01,  5.9674e-02,  4.2432e-01,\n",
      "            8.0976e-01,  1.3307e-01,  1.5413e+00,  6.1810e-01, -3.0199e-01,\n",
      "           -4.1085e-01,  1.2030e+00,  5.4300e-01,  1.8745e+00,  1.6051e+00,\n",
      "           -5.7080e-01, -1.1244e+00,  6.2893e-01],\n",
      "          [ 7.3111e-01, -1.3472e+00, -8.7134e-01,  2.1131e+00,  1.3738e+00,\n",
      "           -2.3250e-01,  1.3802e+00,  1.1664e+00, -5.2593e-01,  6.2756e-02,\n",
      "            1.4208e+00,  1.0314e+00, -6.4101e-01, -1.0028e+00,  8.5794e-01,\n",
      "           -1.9210e-01, -4.7721e-01, -4.7293e-01, -1.7142e+00,  3.9996e-02,\n",
      "            3.9617e-01,  6.7150e-01, -2.7440e-01,  3.1073e-01, -1.7890e-01,\n",
      "           -1.7713e+00, -9.2203e-01, -6.3838e-01],\n",
      "          [ 9.3613e-01,  1.3147e+00,  7.3979e-01,  1.9615e+00,  5.0362e-01,\n",
      "           -2.6092e-01, -7.2922e-01, -7.7395e-01,  7.5246e-01,  7.1466e-01,\n",
      "           -5.2469e-01,  3.2541e-01, -1.6324e-01,  3.3053e-01, -1.0752e+00,\n",
      "            2.2708e+00, -1.0374e+00,  1.1574e+00,  5.3874e-01, -1.0988e+00,\n",
      "           -1.8736e+00, -2.9092e-01, -1.2755e-01,  2.0764e-01, -3.2881e-01,\n",
      "           -7.2951e-01,  2.6314e-03, -3.7394e-01],\n",
      "          [-1.8392e+00,  4.6867e-01,  1.5912e-01, -1.4040e+00, -7.9970e-01,\n",
      "           -9.5639e-01, -5.5082e-01,  3.0784e-01, -3.2537e-01,  8.3578e-01,\n",
      "            4.1936e-01,  1.7929e+00, -4.2587e-01, -1.9510e-01, -2.6044e-01,\n",
      "           -1.7959e-01, -3.6044e-01, -2.5590e-01, -3.9204e-01, -6.3776e-01,\n",
      "            2.2644e-01, -5.7498e-01, -8.8922e-01, -3.6713e-01, -1.0788e+00,\n",
      "            2.7459e-01,  4.8686e-01,  1.3713e+00],\n",
      "          [-8.2790e-01, -8.5048e-01, -2.8914e+00, -3.1585e-01,  7.5744e-02,\n",
      "            1.1485e+00,  1.2425e+00, -8.3670e-02, -3.8917e+00, -1.4415e+00,\n",
      "            9.2393e-01, -1.7160e-01,  7.1859e-01, -1.7140e-01, -3.1760e-01,\n",
      "            1.8723e+00, -4.4634e-01,  1.5996e+00,  1.1711e+00,  5.8461e-01,\n",
      "            4.9264e-01, -6.6548e-01, -1.5173e+00, -1.5877e-01,  2.3384e-01,\n",
      "            1.4874e-01,  7.8471e-02,  9.0769e-01],\n",
      "          [-5.8924e-01, -8.3491e-01,  3.7380e-01, -5.0718e-01, -8.6616e-01,\n",
      "           -7.8286e-01,  2.1893e-01, -2.8762e-01,  1.3813e-01,  1.5620e+00,\n",
      "            9.1954e-01, -6.1094e-01, -1.8483e-01, -9.7604e-01,  4.4767e-01,\n",
      "           -7.3518e-01, -1.8353e+00, -3.5751e-01, -2.5083e-01,  1.3327e+00,\n",
      "           -1.3511e+00, -9.5063e-01, -1.0440e+00, -1.4530e+00,  1.9578e+00,\n",
      "            1.6628e+00,  1.0685e+00, -1.6141e+00],\n",
      "          [ 1.8684e-01, -3.4259e-01,  9.4848e-01, -4.2274e-01,  1.3744e+00,\n",
      "           -7.0530e-01, -3.7143e-01,  1.2994e+00,  3.8283e-01, -5.0954e-01,\n",
      "            3.1435e-01, -3.2281e-01, -1.6212e+00,  3.3284e-01,  2.1521e+00,\n",
      "           -4.7621e-01,  1.0535e-01,  1.1596e-01,  1.2792e+00, -8.0508e-01,\n",
      "           -3.1379e-01,  6.3565e-01,  1.2758e-01, -1.6329e+00, -7.5452e-01,\n",
      "           -9.3403e-01,  7.2922e-01,  7.3400e-02],\n",
      "          [-2.2980e-01,  5.2240e-01, -6.6032e-01, -1.5983e+00,  1.2622e+00,\n",
      "           -8.3383e-01, -4.9103e-01, -6.3131e-01,  6.4824e-01, -1.1529e+00,\n",
      "            1.3382e+00, -7.4660e-01,  1.3897e-01,  2.5636e-01,  4.7402e-01,\n",
      "            4.8386e-01,  6.3824e-01,  4.8927e-01,  9.4253e-01,  5.9657e-01,\n",
      "            2.7742e-01, -1.1125e-01,  1.2818e+00, -1.1252e-01,  1.6062e+00,\n",
      "            6.5279e-02,  6.2034e-01,  1.1019e-01],\n",
      "          [ 2.3660e-02,  1.5542e-01, -1.0386e+00,  1.8259e+00,  6.6471e-02,\n",
      "           -1.7565e+00, -1.9691e-01, -1.4599e+00, -1.4893e-02,  6.9622e-02,\n",
      "           -4.1048e-01,  1.2510e+00, -7.0596e-02,  1.2502e-02, -4.2236e-01,\n",
      "           -1.3198e+00,  3.0807e-01,  1.3496e+00, -2.0163e-01,  1.6688e-01,\n",
      "            3.4904e+00, -3.3616e-01,  1.6445e-01, -1.4451e-01,  1.2362e+00,\n",
      "            2.0946e-01,  1.0969e+00,  1.8494e-01],\n",
      "          [-6.9429e-01,  4.0160e-01, -3.5325e-01,  1.3001e+00,  7.0770e-01,\n",
      "            1.1110e+00,  3.8818e-01, -1.1102e+00,  8.8994e-02,  5.8468e-02,\n",
      "            1.5425e-01,  1.9513e+00,  1.0024e+00,  1.3605e+00,  3.6506e-01,\n",
      "           -2.5126e-01, -1.4597e+00,  9.6095e-02,  3.2438e-02, -6.8287e-01,\n",
      "            6.4690e-01,  1.1240e+00, -3.8465e-01,  2.0712e-01, -5.3960e-01,\n",
      "            6.7211e-01, -2.7583e+00, -1.2723e+00],\n",
      "          [ 5.8057e-01, -1.6213e+00,  5.4568e-01, -5.7127e-01,  1.0453e+00,\n",
      "           -4.9167e-01,  1.0231e+00, -3.8493e-01,  1.6370e+00, -4.2192e-01,\n",
      "            1.8380e-01, -7.9575e-01,  1.8435e+00, -2.2502e-01,  5.1772e-01,\n",
      "           -1.8020e-01, -2.9826e-01, -4.2520e-02, -3.0430e-01, -1.5522e-01,\n",
      "            1.0715e+00, -8.7883e-01, -1.0187e+00, -9.2742e-01,  3.7068e-01,\n",
      "            1.3028e-01,  8.0367e-01,  1.6173e+00],\n",
      "          [ 8.8889e-01, -2.0273e+00, -8.0901e-02,  3.8005e-01,  6.7210e-01,\n",
      "            7.6742e-01, -1.9401e+00, -2.0224e-01,  2.2541e+00, -8.5988e-01,\n",
      "            5.4591e-01, -1.3542e+00,  6.7130e-04, -1.5781e+00,  8.0652e-01,\n",
      "           -2.2328e+00, -9.6492e-02, -1.3714e+00, -9.9665e-01,  6.9244e-01,\n",
      "            7.7573e-01, -3.5609e-01,  1.2088e+00,  3.0363e-01,  4.5596e-01,\n",
      "           -9.9790e-01,  2.0384e+00, -1.1610e+00],\n",
      "          [-1.2754e-02, -6.1965e-01, -2.6757e-01,  5.7655e-02,  1.8150e-01,\n",
      "            5.0418e-01, -2.0224e+00, -1.0417e+00, -1.3340e+00, -1.4178e+00,\n",
      "            1.4925e+00,  9.4228e-01, -3.9382e-01, -1.2339e+00, -1.7065e+00,\n",
      "           -1.0190e+00,  2.9580e+00, -4.5523e-01, -1.1078e-01,  7.9712e-01,\n",
      "           -1.6863e+00, -3.3023e-02, -1.2534e-01, -3.9032e-01, -6.3485e-01,\n",
      "           -5.7470e-01, -1.7210e+00, -3.4236e-01],\n",
      "          [-1.6106e+00, -2.2014e-01,  1.8946e-01,  6.6184e-01, -8.9023e-01,\n",
      "           -2.2311e+00,  6.5213e-01, -6.5038e-03, -1.4035e+00, -1.5308e+00,\n",
      "           -1.6173e-01,  6.2828e-02, -8.4332e-01,  3.5551e-01, -8.9894e-01,\n",
      "           -7.3676e-01, -6.4240e-01, -2.8644e-01, -8.4598e-01, -1.9257e-01,\n",
      "            9.2332e-01, -2.2144e+00,  3.7022e-01,  1.3594e+00, -2.0005e-01,\n",
      "            8.2099e-01,  7.7648e-01,  3.6231e-01],\n",
      "          [ 7.4569e-01,  1.1294e+00, -1.1478e+00,  7.3786e-02,  1.2909e+00,\n",
      "           -3.8997e-01, -8.6691e-01, -1.8476e+00, -4.0272e-01, -6.5290e-01,\n",
      "           -3.7855e-01, -5.4895e-01,  7.2845e-01, -6.2728e-01, -3.4045e-01,\n",
      "            7.5512e-01,  2.9360e-01, -1.3242e+00, -4.6986e-01, -3.7083e-01,\n",
      "            5.5378e-01,  2.2568e-01, -2.7114e-01, -3.0406e-02, -1.2020e+00,\n",
      "            3.3338e-01, -1.4861e+00,  8.9346e-01],\n",
      "          [ 2.6523e+00, -3.1049e-01, -2.9607e-01,  4.6787e-01,  6.8885e-02,\n",
      "           -5.9440e-01, -1.7978e+00, -5.6717e-01, -1.1128e+00, -1.5158e+00,\n",
      "            2.5254e-01, -2.6617e-01, -8.1718e-01, -1.6593e+00,  4.9917e-01,\n",
      "           -5.7857e-01, -3.1636e-01,  4.1944e-01, -6.4440e-01, -3.7530e-01,\n",
      "           -1.9014e+00, -7.2836e-01,  2.9883e-01, -1.3288e-01,  2.2951e+00,\n",
      "           -3.3253e-01,  1.2663e+00, -5.8790e-01],\n",
      "          [ 1.2694e-01,  3.6132e-01, -9.8712e-01, -8.0242e-01, -1.1577e+00,\n",
      "            2.0374e+00, -2.6747e-01,  8.3271e-01,  1.4699e+00,  4.7256e-03,\n",
      "           -9.9325e-01, -3.1159e-01,  3.0783e-01, -9.5981e-01, -8.0828e-02,\n",
      "            5.9371e-01,  6.4809e-01,  2.8198e-01,  8.2399e-01,  4.1541e-01,\n",
      "           -3.4241e-01, -1.0018e+00,  1.6356e-01,  3.1362e-01,  1.1602e+00,\n",
      "           -1.1549e+00, -7.8261e-01,  1.5912e+00],\n",
      "          [-7.1215e-01, -1.3912e+00,  4.8908e-01, -1.0109e+00, -5.2697e-02,\n",
      "            1.4288e+00,  9.8029e-01,  6.9220e-01, -1.1166e+00,  1.7977e+00,\n",
      "            3.9417e-01,  5.5717e-01, -8.1728e-01,  7.1938e-02,  5.2814e-01,\n",
      "            2.3127e+00, -9.9783e-01,  2.0789e+00, -7.3179e-02, -1.7476e+00,\n",
      "            3.1926e-01,  7.1544e-03, -1.1273e+00, -7.5031e-01,  7.3084e-01,\n",
      "           -1.5111e-02,  3.0182e-01, -1.6202e+00],\n",
      "          [ 1.6571e-01, -2.7305e-01, -3.3964e-02, -2.2663e+00, -1.0493e+00,\n",
      "            2.0539e+00, -7.5724e-01, -1.7381e-01, -1.6285e+00, -6.1371e-01,\n",
      "           -8.0504e-01,  6.5451e-01,  1.0036e+00,  2.3048e-01, -6.4164e-01,\n",
      "           -3.1913e-01, -1.3649e+00,  8.4552e-01, -2.7675e-02, -6.9771e-01,\n",
      "            5.1903e-01,  5.2074e-01,  1.4002e-01, -6.2971e-01,  9.3926e-01,\n",
      "           -1.1054e+00, -9.1875e-01, -2.6674e-01],\n",
      "          [-1.0306e+00,  5.1236e-01,  7.9590e-01, -6.9918e-01, -3.8898e-01,\n",
      "           -2.6226e-03,  1.3384e+00,  4.7612e-01,  8.7328e-01,  1.0843e+00,\n",
      "           -2.6413e-01,  1.3071e+00,  2.5881e-01,  3.0795e-01, -1.9954e+00,\n",
      "            2.2228e-01,  1.3072e+00, -2.2137e+00,  9.0322e-01,  3.2753e-01,\n",
      "           -4.0410e-01,  1.8277e+00, -1.1485e-02,  3.9796e-02,  1.4214e+00,\n",
      "           -1.3245e+00,  9.9944e-01,  1.4726e+00],\n",
      "          [ 1.8882e-02, -3.1840e-01,  1.0691e+00, -1.2801e+00, -8.3140e-02,\n",
      "           -1.3922e+00,  2.4136e-02,  6.3560e-01,  4.9327e-01,  9.1865e-01,\n",
      "           -9.1830e-01,  8.5748e-01,  1.3069e+00, -9.0322e-01, -4.5528e-01,\n",
      "            6.3233e-01, -1.1784e+00,  2.2404e-01,  2.8942e-01,  2.9817e-03,\n",
      "           -3.6981e-01,  5.7576e-01,  5.9628e-01,  1.6609e+00,  2.6024e-01,\n",
      "            2.2146e+00, -1.7670e+00, -4.4304e-01],\n",
      "          [-8.2050e-01, -6.4987e-01, -4.9132e-01,  6.5496e-02,  8.7929e-01,\n",
      "            1.9791e+00, -1.3532e+00,  4.3548e-02, -1.2974e-01,  1.6044e+00,\n",
      "            1.1308e+00,  9.0222e-01, -1.4681e+00,  1.1669e+00, -2.1232e+00,\n",
      "           -9.1519e-01, -4.8778e-01, -2.1384e+00,  1.8644e+00,  6.2020e-01,\n",
      "            1.4768e-01,  8.9713e-01, -3.8333e-01,  5.3428e-01, -9.5933e-01,\n",
      "            9.9257e-01, -1.9364e+00,  5.5410e-01],\n",
      "          [-1.4047e-01, -8.5844e-01, -5.8106e-01, -4.2940e-01, -5.8349e-03,\n",
      "            1.2851e+00, -9.4723e-01, -1.0202e+00, -9.3156e-01,  5.2106e-01,\n",
      "           -1.2527e+00,  1.8460e-01, -3.8662e-01,  3.9658e-01,  3.6133e-01,\n",
      "            4.0812e-01,  1.6193e+00, -5.3820e-01,  4.5061e-01,  9.9611e-01,\n",
      "            2.4829e-01,  8.2100e-01, -2.7576e-01, -7.1842e-01, -1.9217e+00,\n",
      "           -1.3826e+00,  6.0998e-01,  2.0643e-01],\n",
      "          [ 9.7481e-01,  6.1516e-01,  2.9178e-01, -5.4222e-01,  7.2809e-02,\n",
      "            6.4826e-01,  7.0795e-01, -4.6437e-01,  5.4432e-01,  2.9156e+00,\n",
      "           -1.4718e-01,  2.8186e-01, -1.4840e+00, -2.0601e+00, -9.1789e-02,\n",
      "           -8.4408e-01,  2.3114e+00,  5.7476e-02,  3.3401e-01, -2.5308e-01,\n",
      "           -1.8847e+00, -1.8806e-01,  7.5381e-01,  2.1577e-01,  7.7877e-01,\n",
      "           -1.8664e+00, -9.3509e-01,  1.6421e+00],\n",
      "          [ 1.5276e+00, -6.0925e-01,  9.8439e-01, -4.5173e-01,  2.3551e-01,\n",
      "            5.6270e-01, -6.8846e-01,  7.2656e-01,  2.7718e+00,  8.7906e-01,\n",
      "           -4.9408e-01,  1.1388e+00,  1.3820e+00,  7.3318e-01, -8.2097e-01,\n",
      "           -7.5291e-01,  8.7261e-01, -2.3314e+00,  8.1390e-01,  4.8170e-01,\n",
      "           -8.0039e-01,  1.1946e+00, -8.0458e-02,  1.7586e+00, -2.4851e-02,\n",
      "            7.5643e-01, -1.2937e+00,  8.1580e-01],\n",
      "          [ 2.0707e+00,  2.0706e-01, -4.3980e-01, -5.2342e-01,  9.3151e-01,\n",
      "           -7.0297e-02,  9.5680e-01,  4.5934e-01,  1.8107e+00, -1.0489e-02,\n",
      "           -2.6718e-01,  1.5993e-01, -1.1253e+00,  4.1020e-01, -2.6857e+00,\n",
      "           -6.6705e-01,  2.7519e+00,  5.7221e-02, -3.5238e-01, -4.0302e-01,\n",
      "           -2.5717e-01, -1.0416e-01, -4.5823e-01, -6.5937e-01,  8.1157e-02,\n",
      "           -1.6726e-01, -4.0106e-01, -3.6153e-02]]]])\n",
      "tensor([[[[ 2.2178e-01, -1.5979e-01, -8.3926e-01, -1.6320e+00, -7.3455e-01,\n",
      "            2.2802e+00, -1.1636e+00, -4.9150e-01, -6.3017e-01,  1.1051e-02,\n",
      "           -1.8590e-01, -2.8710e-01, -6.9774e-01,  9.1433e-01, -1.0705e+00,\n",
      "           -1.3713e+00, -6.7129e-01,  3.0552e-01, -5.5734e-01,  1.2205e-01,\n",
      "           -9.5071e-01,  4.0515e-01,  1.5153e+00,  9.4600e-01,  5.5926e-01,\n",
      "            2.7798e-01,  1.4532e+00,  4.4996e-01],\n",
      "          [ 1.1130e+00,  1.1189e+00,  1.0048e+00, -4.6634e-01,  1.5646e+00,\n",
      "           -8.9534e-01, -7.2577e-01, -1.2421e+00,  6.5713e-01,  4.9168e-01,\n",
      "            3.8580e-01,  6.3497e-01, -1.9127e+00, -8.3342e-01,  2.9912e-01,\n",
      "            9.8411e-01, -5.1726e-02,  1.1211e+00, -1.3875e+00, -1.9627e+00,\n",
      "           -8.4804e-01, -1.2338e+00,  2.4521e+00,  2.2840e-02,  1.1712e+00,\n",
      "            3.3673e-01,  4.5254e-02, -4.0776e-01],\n",
      "          [-9.8727e-02,  1.0801e+00, -8.1883e-01,  1.6538e+00,  5.8883e-01,\n",
      "           -5.1440e-01,  1.1507e+00, -6.5262e-02, -1.8095e+00, -4.6491e-01,\n",
      "            1.5105e+00,  1.0738e+00,  2.4817e-01,  1.4469e-01,  5.3039e-01,\n",
      "            7.9093e-01,  3.2484e-01,  1.3972e+00,  7.0803e-01, -2.9125e-01,\n",
      "           -4.7579e-01,  1.2903e+00,  5.7397e-01,  1.8387e+00,  1.5641e+00,\n",
      "           -5.8092e-01, -1.1438e+00,  5.0618e-01],\n",
      "          [ 7.6544e-01, -1.4246e+00, -9.7150e-01,  2.3760e+00,  1.3810e+00,\n",
      "           -6.0202e-02,  1.3605e+00,  1.1783e+00, -4.0344e-01, -4.8344e-02,\n",
      "            1.3508e+00,  9.6737e-01, -8.0756e-01, -8.3786e-01,  8.3615e-01,\n",
      "            4.4064e-03, -4.6683e-01, -3.6141e-01, -1.9036e+00,  4.3783e-02,\n",
      "            5.3998e-01,  6.8740e-01, -2.8349e-01,  2.9045e-01, -1.6173e-01,\n",
      "           -1.6908e+00, -9.3400e-01, -7.7966e-01],\n",
      "          [ 8.5843e-01,  1.2431e+00,  6.9880e-01,  1.9998e+00,  6.5372e-01,\n",
      "           -5.8798e-02, -8.1466e-01, -1.0055e+00,  7.5748e-01,  8.2964e-01,\n",
      "           -4.4268e-01,  4.6793e-01, -1.5996e-01,  4.7445e-01, -9.2776e-01,\n",
      "            2.2753e+00, -1.0453e+00,  1.1905e+00,  5.6293e-01, -1.1035e+00,\n",
      "           -1.9042e+00, -2.3223e-01, -3.6418e-01,  2.9308e-01, -4.7188e-01,\n",
      "           -7.9308e-01,  1.1079e-01, -4.1414e-01],\n",
      "          [-1.7974e+00,  4.4275e-01, -3.3829e-03, -1.2582e+00, -8.4875e-01,\n",
      "           -8.3738e-01, -5.8622e-01,  3.5776e-01, -2.3030e-01,  8.9058e-01,\n",
      "            4.2265e-01,  1.8845e+00, -3.7171e-01, -2.8912e-01, -2.3789e-01,\n",
      "           -2.9740e-01, -3.8838e-01, -2.7260e-01, -4.0067e-01, -5.9670e-01,\n",
      "            2.4461e-01, -3.6810e-01, -7.5619e-01, -3.8665e-01, -1.2195e+00,\n",
      "            4.6734e-01,  4.7254e-01,  1.2583e+00],\n",
      "          [-8.8279e-01, -9.6546e-01, -2.6995e+00, -3.4228e-01,  1.0114e-02,\n",
      "            1.2526e+00,  1.0481e+00, -7.4324e-02, -3.9904e+00, -1.5184e+00,\n",
      "            8.5196e-01, -2.4822e-01,  6.8247e-01, -2.2322e-01, -3.1518e-01,\n",
      "            1.9343e+00, -3.7109e-01,  1.7099e+00,  1.1931e+00,  6.8870e-01,\n",
      "            4.8076e-01, -6.8612e-01, -1.6947e+00, -4.5499e-02,  2.9064e-01,\n",
      "            2.4757e-01, -2.0175e-01,  7.5903e-01],\n",
      "          [-4.1705e-01, -9.4113e-01,  1.1980e-01, -4.6824e-01, -8.1316e-01,\n",
      "           -7.7837e-01,  3.8770e-01, -2.6573e-01,  1.6109e-01,  1.6874e+00,\n",
      "            8.8813e-01, -5.5022e-01, -4.8648e-01, -8.0854e-01,  4.2735e-01,\n",
      "           -7.5347e-01, -2.0672e+00, -2.9098e-01, -2.5705e-01,  1.3576e+00,\n",
      "           -1.3536e+00, -8.8600e-01, -1.1782e+00, -1.5765e+00,  1.9330e+00,\n",
      "            1.7109e+00,  1.0046e+00, -1.6529e+00],\n",
      "          [-3.5985e-02, -4.1660e-01,  8.0368e-01, -4.2913e-01,  1.4455e+00,\n",
      "           -7.9911e-01, -1.1886e-01,  1.4106e+00,  2.1980e-01, -4.4833e-01,\n",
      "            3.2393e-01, -3.6056e-01, -1.4953e+00,  5.2486e-01,  2.0837e+00,\n",
      "           -4.6412e-01,  6.2434e-02, -8.3467e-02,  1.1848e+00, -8.7876e-01,\n",
      "           -3.4003e-01,  7.5517e-01,  2.3290e-01, -1.6270e+00, -6.7704e-01,\n",
      "           -9.7272e-01,  7.2200e-01,  2.4761e-01],\n",
      "          [-3.0535e-01,  5.6114e-01, -7.1040e-01, -1.5714e+00,  1.3191e+00,\n",
      "           -6.7876e-01, -6.0876e-01, -6.4732e-01,  6.4035e-01, -1.0336e+00,\n",
      "            1.4276e+00, -8.0828e-01, -7.0116e-02,  2.1245e-01,  3.6034e-01,\n",
      "            4.9075e-01,  6.9561e-01,  6.3442e-01,  9.4294e-01,  6.3963e-01,\n",
      "            2.2045e-01, -1.0697e-01,  1.3632e+00, -9.6378e-02,  1.4825e+00,\n",
      "            8.8648e-03,  7.5676e-01,  2.1496e-01],\n",
      "          [-1.1682e-01, -9.1567e-03, -1.0712e+00,  2.0043e+00, -7.3643e-02,\n",
      "           -1.7474e+00, -1.6421e-01, -1.5734e+00, -2.8816e-02,  1.5025e-01,\n",
      "           -5.2132e-01,  1.2464e+00, -2.0764e-01,  8.3587e-02, -4.1724e-01,\n",
      "           -1.3711e+00,  6.0895e-02,  1.4436e+00, -2.4038e-01,  1.9102e-01,\n",
      "            3.3008e+00, -3.8051e-01,  5.3490e-02, -9.9330e-02,  1.1602e+00,\n",
      "            3.3457e-01,  1.0420e+00,  2.0322e-01],\n",
      "          [-6.2850e-01,  4.0970e-01, -1.7123e-01,  1.3250e+00,  7.3624e-01,\n",
      "            1.3079e+00,  3.0706e-01, -1.2816e+00,  1.0659e-01,  9.7071e-02,\n",
      "            1.7930e-01,  1.8421e+00,  1.0174e+00,  1.3170e+00,  2.4472e-01,\n",
      "           -2.3086e-01, -1.3541e+00,  1.0118e-01,  9.1418e-02, -5.8437e-01,\n",
      "            6.4535e-01,  1.2940e+00, -3.7629e-01,  1.4788e-01, -4.8029e-01,\n",
      "            7.2863e-01, -2.8809e+00, -1.3388e+00],\n",
      "          [ 4.4713e-01, -1.6605e+00,  6.0107e-01, -4.9464e-01,  9.4189e-01,\n",
      "           -5.4758e-01,  8.9035e-01, -4.2756e-01,  1.4965e+00, -3.9934e-01,\n",
      "            2.4990e-01, -8.5536e-01,  1.7270e+00, -2.4654e-01,  5.5821e-01,\n",
      "           -1.9784e-01, -2.6832e-01,  5.9940e-03, -2.9059e-01, -2.0488e-01,\n",
      "            1.1262e+00, -8.8889e-01, -8.8555e-01, -7.8557e-01,  1.8068e-01,\n",
      "            1.6750e-01,  1.0004e+00,  1.4686e+00],\n",
      "          [ 9.1106e-01, -2.0770e+00, -1.3478e-02,  2.9830e-01,  6.0912e-01,\n",
      "            8.0083e-01, -1.8257e+00, -8.4314e-02,  2.2420e+00, -8.2917e-01,\n",
      "            4.0075e-01, -1.2219e+00,  2.5105e-01, -1.6389e+00,  7.7480e-01,\n",
      "           -2.3060e+00, -1.4291e-02, -1.4623e+00, -1.0831e+00,  7.3836e-01,\n",
      "            9.7539e-01, -2.4028e-01,  1.1297e+00,  2.3878e-01,  2.4121e-01,\n",
      "           -1.0722e+00,  1.9890e+00, -1.2156e+00],\n",
      "          [-5.8227e-02, -6.6780e-01, -3.5512e-01, -1.1335e-01,  6.0551e-02,\n",
      "            6.5356e-01, -1.9353e+00, -1.2933e+00, -1.3286e+00, -1.4162e+00,\n",
      "            1.3999e+00,  8.4241e-01, -5.4156e-01, -1.3004e+00, -1.7179e+00,\n",
      "           -9.4275e-01,  2.7966e+00, -6.0220e-01, -2.2475e-01,  9.4648e-01,\n",
      "           -1.7276e+00,  8.4506e-03,  6.4456e-02, -1.9751e-01, -6.7685e-01,\n",
      "           -4.2941e-01, -1.8006e+00, -4.8611e-01],\n",
      "          [-1.6604e+00, -1.2538e-01,  2.4434e-01,  7.0641e-01, -7.5573e-01,\n",
      "           -2.2945e+00,  6.7366e-01, -5.0915e-02, -1.4774e+00, -1.4632e+00,\n",
      "           -2.2517e-01, -3.9185e-02, -1.2029e+00,  2.7295e-01, -9.3966e-01,\n",
      "           -7.6813e-01, -6.3921e-01, -2.7268e-01, -6.3078e-01, -3.2998e-01,\n",
      "            8.3008e-01, -2.3988e+00,  3.0707e-01,  1.4190e+00, -2.2610e-01,\n",
      "            7.2531e-01,  8.0187e-01,  3.9683e-01],\n",
      "          [ 8.4016e-01,  1.0111e+00, -1.0492e+00,  2.1799e-03,  1.1772e+00,\n",
      "           -3.4899e-01, -1.1810e+00, -1.9969e+00, -4.6268e-01, -7.1029e-01,\n",
      "           -3.5232e-01, -7.7449e-01,  7.4500e-01, -7.0994e-01, -3.3085e-01,\n",
      "            8.2505e-01,  2.0959e-01, -1.3472e+00, -5.9515e-01, -4.0543e-01,\n",
      "            5.0970e-01,  3.0111e-01, -1.5472e-01, -1.6302e-01, -1.3979e+00,\n",
      "            3.6039e-01, -1.5044e+00,  8.7089e-01],\n",
      "          [ 2.6833e+00, -3.2194e-01, -1.8106e-01,  4.5126e-01,  1.3078e-01,\n",
      "           -4.6889e-01, -1.8609e+00, -6.0525e-01, -1.0620e+00, -1.4002e+00,\n",
      "            2.9601e-01, -2.5800e-01, -9.4935e-01, -1.7199e+00,  4.0813e-01,\n",
      "           -5.0928e-01, -4.1947e-01,  3.4944e-01, -7.8947e-01, -4.5141e-01,\n",
      "           -1.8255e+00, -8.4199e-01,  2.8010e-01, -1.6403e-01,  2.1548e+00,\n",
      "           -2.8598e-01,  1.3376e+00, -4.6247e-01],\n",
      "          [ 1.2584e-01,  3.7323e-01, -9.5730e-01, -7.0397e-01, -1.2069e+00,\n",
      "            2.1405e+00, -2.7706e-01,  8.3707e-01,  1.4474e+00,  1.4395e-01,\n",
      "           -1.0200e+00, -3.3120e-01,  2.5964e-01, -9.5649e-01, -1.8637e-01,\n",
      "            8.3551e-01,  5.3418e-01,  3.0740e-01,  7.6176e-01,  6.1664e-01,\n",
      "           -2.5123e-01, -9.8622e-01,  6.8956e-02,  2.6567e-01,  1.2048e+00,\n",
      "           -1.3515e+00, -7.7035e-01,  1.6861e+00],\n",
      "          [-6.5777e-01, -1.3345e+00,  3.9688e-01, -9.3920e-01,  3.6564e-02,\n",
      "            1.4083e+00,  1.0372e+00,  8.2169e-01, -1.0986e+00,  1.9583e+00,\n",
      "            3.2662e-01,  5.7607e-01, -7.7760e-01,  1.3044e-02,  4.7453e-01,\n",
      "            2.3157e+00, -8.9807e-01,  1.8860e+00,  3.6270e-02, -1.8079e+00,\n",
      "            3.9459e-01,  2.5335e-03, -1.2302e+00, -6.8437e-01,  7.5449e-01,\n",
      "            1.0939e-01,  3.2057e-01, -1.6049e+00],\n",
      "          [ 3.0454e-01, -3.7278e-01, -3.5979e-01, -2.2361e+00, -1.2844e+00,\n",
      "            2.0882e+00, -6.5127e-01, -3.2886e-01, -1.5229e+00, -6.6869e-01,\n",
      "           -7.1041e-01,  5.9743e-01,  8.9492e-01,  2.1318e-01, -7.1481e-01,\n",
      "           -2.7909e-01, -1.4440e+00,  8.4734e-01, -1.8854e-03, -7.2953e-01,\n",
      "            6.4635e-01,  6.2422e-01,  1.7203e-01, -4.7170e-01,  9.8552e-01,\n",
      "           -1.1262e+00, -9.5265e-01, -2.7652e-01],\n",
      "          [-1.1960e+00,  3.9613e-01,  6.1880e-01, -7.2161e-01, -4.2171e-01,\n",
      "            2.0419e-02,  1.1319e+00,  3.6352e-01,  7.6223e-01,  1.2240e+00,\n",
      "           -1.8501e-01,  1.3607e+00,  2.6243e-01,  3.0219e-01, -1.8983e+00,\n",
      "            9.0733e-02,  1.1987e+00, -2.1356e+00,  9.7184e-01,  4.0231e-01,\n",
      "           -4.9425e-01,  1.8536e+00,  8.5872e-02,  2.8007e-02,  1.3978e+00,\n",
      "           -1.2190e+00,  9.6862e-01,  1.4829e+00],\n",
      "          [ 3.1208e-02, -2.4607e-01,  9.6815e-01, -1.2734e+00, -6.4223e-03,\n",
      "           -1.3092e+00,  2.5311e-02,  7.4609e-01,  5.1792e-01,  8.2163e-01,\n",
      "           -9.6014e-01,  8.1952e-01,  1.2198e+00, -1.0659e+00, -3.0627e-01,\n",
      "            7.2230e-01, -1.1566e+00,  3.6147e-01,  7.2278e-02,  3.1188e-02,\n",
      "           -2.7461e-01,  5.6413e-01,  6.8209e-01,  1.7632e+00,  9.0034e-02,\n",
      "            2.0954e+00, -1.7821e+00, -5.5134e-01],\n",
      "          [-8.0220e-01, -5.6738e-01, -5.6899e-01,  7.5388e-02,  7.9279e-01,\n",
      "            2.0520e+00, -1.2047e+00,  6.8525e-02, -2.1714e-01,  1.4902e+00,\n",
      "            1.2060e+00,  8.7840e-01, -1.3228e+00,  1.2596e+00, -2.1442e+00,\n",
      "           -1.0126e+00, -3.9725e-01, -2.1792e+00,  2.0829e+00,  7.5427e-01,\n",
      "            3.2616e-01,  9.8673e-01, -2.4562e-01,  5.1472e-01, -1.1110e+00,\n",
      "            8.9195e-01, -1.9425e+00,  6.4066e-01],\n",
      "          [-6.9254e-02, -6.5979e-01, -5.6824e-01, -6.8060e-01,  2.8164e-02,\n",
      "            1.3750e+00, -1.0570e+00, -8.9031e-01, -1.1678e+00,  5.9867e-01,\n",
      "           -1.3861e+00,  2.7704e-01, -3.7738e-01,  3.2367e-01,  3.9732e-01,\n",
      "            4.9728e-01,  1.5801e+00, -5.7470e-01,  5.3003e-01,  1.1655e+00,\n",
      "            1.0424e-01,  7.6547e-01, -4.7235e-01, -5.7424e-01, -1.7075e+00,\n",
      "           -1.4957e+00,  5.8706e-01,  1.1603e-01],\n",
      "          [ 9.2437e-01,  7.9381e-01,  8.3462e-02, -4.5191e-01,  2.9526e-02,\n",
      "            8.6981e-01,  7.9505e-01, -4.1779e-01,  5.5078e-01,  2.8034e+00,\n",
      "           -2.5858e-01,  2.3061e-01, -1.4651e+00, -2.1754e+00, -4.8937e-03,\n",
      "           -6.7911e-01,  2.2719e+00,  1.4796e-02,  5.5761e-01, -3.1359e-01,\n",
      "           -2.0233e+00, -7.4475e-02,  8.0862e-01,  2.1262e-01,  6.8185e-01,\n",
      "           -1.8931e+00, -1.0773e+00,  1.6207e+00],\n",
      "          [ 1.5189e+00, -5.5167e-01,  8.7680e-01, -4.4601e-01,  8.8847e-02,\n",
      "            4.7562e-01, -4.8537e-01,  5.4671e-01,  2.7049e+00,  8.3670e-01,\n",
      "           -5.6535e-01,  1.2241e+00,  1.3604e+00,  6.9612e-01, -7.7075e-01,\n",
      "           -7.5666e-01,  8.8854e-01, -2.4736e+00,  6.8264e-01,  5.6319e-01,\n",
      "           -7.4827e-01,  1.2800e+00, -1.6898e-01,  1.8099e+00, -3.6760e-02,\n",
      "            5.5488e-01, -1.5401e+00,  8.9650e-01],\n",
      "          [ 2.1141e+00,  2.4804e-01, -5.9817e-01, -4.3789e-01,  9.7952e-01,\n",
      "            6.8796e-03,  8.5522e-01,  3.9159e-01,  1.7810e+00,  8.4744e-04,\n",
      "           -2.2212e-01,  1.7104e-01, -1.1015e+00,  4.7739e-01, -2.6890e+00,\n",
      "           -7.4899e-01,  2.8446e+00,  1.7891e-01, -4.8893e-01, -3.0274e-01,\n",
      "           -3.3213e-01, -8.9239e-02, -5.2412e-01, -5.3676e-01,  1.4358e-01,\n",
      "           -9.7926e-02, -3.5076e-01, -2.0040e-02]]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 2.3084e-01, -1.5820e-01, -8.5139e-01, -1.6516e+00, -7.1817e-01,\n",
      "            2.2906e+00, -1.1556e+00, -4.8849e-01, -6.2959e-01,  2.4124e-03,\n",
      "           -1.9445e-01, -2.9305e-01, -6.9319e-01,  9.0015e-01, -1.0702e+00,\n",
      "           -1.3728e+00, -6.7088e-01,  3.0273e-01, -5.8067e-01,  1.3279e-01,\n",
      "           -9.4047e-01,  4.1332e-01,  1.5097e+00,  9.5824e-01,  5.4256e-01,\n",
      "            2.8656e-01,  1.4539e+00,  4.4931e-01],\n",
      "          [ 1.1215e+00,  1.1187e+00,  9.9126e-01, -4.6029e-01,  1.5452e+00,\n",
      "           -8.9443e-01, -7.3710e-01, -1.2352e+00,  6.5247e-01,  4.9716e-01,\n",
      "            3.8489e-01,  6.4389e-01, -1.9342e+00, -8.3995e-01,  2.9585e-01,\n",
      "            9.7763e-01, -6.7083e-02,  1.1108e+00, -1.3868e+00, -1.9769e+00,\n",
      "           -8.4924e-01, -1.2275e+00,  2.4498e+00,  2.2762e-02,  1.1707e+00,\n",
      "            3.4780e-01,  3.2049e-02, -4.2713e-01],\n",
      "          [-1.1414e-01,  1.0735e+00, -8.2235e-01,  1.6663e+00,  5.8086e-01,\n",
      "           -5.0908e-01,  1.1549e+00, -6.8086e-02, -1.7907e+00, -4.7130e-01,\n",
      "            1.5111e+00,  1.0877e+00,  2.4652e-01,  1.4030e-01,  5.2141e-01,\n",
      "            7.7947e-01,  3.1245e-01,  1.4029e+00,  6.9093e-01, -2.7920e-01,\n",
      "           -4.5770e-01,  1.3067e+00,  5.5733e-01,  1.8369e+00,  1.5725e+00,\n",
      "           -5.6742e-01, -1.1341e+00,  5.0626e-01],\n",
      "          [ 7.6138e-01, -1.4040e+00, -9.4852e-01,  2.3604e+00,  1.3726e+00,\n",
      "           -3.7213e-02,  1.3574e+00,  1.1916e+00, -4.0307e-01, -3.4483e-02,\n",
      "            1.3447e+00,  9.7568e-01, -8.0882e-01, -8.4122e-01,  8.3128e-01,\n",
      "           -1.7255e-02, -4.6221e-01, -3.6126e-01, -1.8756e+00,  3.8633e-02,\n",
      "            5.2538e-01,  6.7700e-01, -2.7889e-01,  3.0216e-01, -1.7122e-01,\n",
      "           -1.6858e+00, -9.2731e-01, -7.7702e-01],\n",
      "          [ 8.6386e-01,  1.2466e+00,  7.1281e-01,  2.0100e+00,  6.8170e-01,\n",
      "           -4.9226e-02, -8.2624e-01, -1.0080e+00,  7.4597e-01,  8.3705e-01,\n",
      "           -4.4047e-01,  4.5500e-01, -1.6360e-01,  4.6450e-01, -9.3982e-01,\n",
      "            2.2670e+00, -1.0217e+00,  1.1857e+00,  5.6385e-01, -1.1011e+00,\n",
      "           -1.9117e+00, -2.2596e-01, -3.6732e-01,  2.9484e-01, -4.8826e-01,\n",
      "           -7.8907e-01,  1.0351e-01, -3.9936e-01],\n",
      "          [-1.7838e+00,  4.4009e-01, -4.0081e-03, -1.2441e+00, -8.2517e-01,\n",
      "           -8.5664e-01, -5.7804e-01,  3.5117e-01, -2.2198e-01,  8.8182e-01,\n",
      "            4.1201e-01,  1.8764e+00, -3.6921e-01, -2.8937e-01, -2.2968e-01,\n",
      "           -3.0214e-01, -3.8301e-01, -2.8044e-01, -4.0331e-01, -6.0768e-01,\n",
      "            2.5292e-01, -3.7171e-01, -7.8074e-01, -3.9153e-01, -1.2071e+00,\n",
      "            4.7816e-01,  4.7474e-01,  1.2663e+00],\n",
      "          [-8.8411e-01, -9.5967e-01, -2.7016e+00, -3.4233e-01, -2.4090e-03,\n",
      "            1.2386e+00,  1.0482e+00, -7.6901e-02, -3.9758e+00, -1.5262e+00,\n",
      "            8.4785e-01, -2.6588e-01,  6.8418e-01, -2.2158e-01, -3.1604e-01,\n",
      "            1.9326e+00, -3.6569e-01,  1.7218e+00,  1.2017e+00,  7.0339e-01,\n",
      "            4.7719e-01, -6.8846e-01, -1.6989e+00, -6.2290e-02,  2.8079e-01,\n",
      "            2.3653e-01, -1.8455e-01,  7.4785e-01],\n",
      "          [-4.0991e-01, -9.4120e-01,  1.2260e-01, -4.6312e-01, -7.9859e-01,\n",
      "           -7.7314e-01,  4.0357e-01, -2.6221e-01,  1.5844e-01,  1.6680e+00,\n",
      "            8.7481e-01, -5.4014e-01, -5.1014e-01, -7.9834e-01,  4.3936e-01,\n",
      "           -7.7119e-01, -2.0678e+00, -2.8367e-01, -2.5966e-01,  1.3528e+00,\n",
      "           -1.3442e+00, -8.9828e-01, -1.1715e+00, -1.5680e+00,  1.9310e+00,\n",
      "            1.7166e+00,  1.0255e+00, -1.6506e+00],\n",
      "          [-2.0498e-02, -4.0451e-01,  7.9628e-01, -4.3420e-01,  1.4569e+00,\n",
      "           -8.0131e-01, -1.2757e-01,  1.4093e+00,  2.3088e-01, -4.3857e-01,\n",
      "            3.2629e-01, -3.7534e-01, -1.4830e+00,  5.2006e-01,  2.0967e+00,\n",
      "           -4.4821e-01,  6.8629e-02, -8.2177e-02,  1.2049e+00, -8.8220e-01,\n",
      "           -3.4232e-01,  7.5482e-01,  2.4461e-01, -1.6353e+00, -6.5509e-01,\n",
      "           -9.7346e-01,  7.1883e-01,  2.3794e-01],\n",
      "          [-3.0524e-01,  5.6485e-01, -7.0849e-01, -1.5769e+00,  1.3190e+00,\n",
      "           -6.9144e-01, -6.1623e-01, -6.4631e-01,  6.5634e-01, -1.0421e+00,\n",
      "            1.4358e+00, -8.0832e-01, -7.0751e-02,  2.2184e-01,  3.3204e-01,\n",
      "            5.1357e-01,  6.7092e-01,  6.4887e-01,  9.3932e-01,  6.4736e-01,\n",
      "            2.0280e-01, -9.8792e-02,  1.3629e+00, -8.3439e-02,  1.4779e+00,\n",
      "            8.4914e-03,  7.3597e-01,  1.9360e-01],\n",
      "          [-1.3290e-01, -1.3044e-02, -1.0765e+00,  2.0180e+00, -7.4940e-02,\n",
      "           -1.7387e+00, -1.5128e-01, -1.5655e+00, -1.4316e-02,  1.5224e-01,\n",
      "           -5.3021e-01,  1.2533e+00, -2.0279e-01,  9.9822e-02, -4.1919e-01,\n",
      "           -1.3714e+00,  5.1839e-02,  1.4384e+00, -2.4176e-01,  1.8939e-01,\n",
      "            3.2963e+00, -3.6247e-01,  4.6028e-02, -9.8989e-02,  1.1449e+00,\n",
      "            3.3268e-01,  1.0439e+00,  1.9615e-01],\n",
      "          [-6.1170e-01,  4.1550e-01, -1.7849e-01,  1.3113e+00,  7.3811e-01,\n",
      "            1.3136e+00,  3.0561e-01, -1.2786e+00,  9.6341e-02,  9.6066e-02,\n",
      "            1.9055e-01,  1.8461e+00,  1.0287e+00,  1.3005e+00,  2.5129e-01,\n",
      "           -2.2843e-01, -1.3590e+00,  1.0789e-01,  9.8182e-02, -5.8162e-01,\n",
      "            6.5274e-01,  1.2877e+00, -3.8787e-01,  1.3501e-01, -4.9384e-01,\n",
      "            7.4562e-01, -2.8722e+00, -1.3534e+00],\n",
      "          [ 4.5555e-01, -1.6500e+00,  6.0789e-01, -5.0381e-01,  9.3959e-01,\n",
      "           -5.3339e-01,  8.9759e-01, -4.1733e-01,  1.5048e+00, -3.9467e-01,\n",
      "            2.4970e-01, -8.6132e-01,  1.7157e+00, -2.5478e-01,  5.5654e-01,\n",
      "           -1.9736e-01, -2.6115e-01,  1.0432e-02, -2.8266e-01, -2.0876e-01,\n",
      "            1.1214e+00, -8.9083e-01, -8.8573e-01, -7.9283e-01,  1.8082e-01,\n",
      "            1.5841e-01,  1.0070e+00,  1.4886e+00],\n",
      "          [ 8.9316e-01, -2.0819e+00,  9.1882e-05,  2.9555e-01,  6.1018e-01,\n",
      "            7.8431e-01, -1.8273e+00, -7.3301e-02,  2.2421e+00, -8.2556e-01,\n",
      "            3.9294e-01, -1.2144e+00,  2.3391e-01, -1.6397e+00,  7.7063e-01,\n",
      "           -2.3205e+00, -1.7469e-02, -1.4700e+00, -1.0793e+00,  7.2825e-01,\n",
      "            9.6715e-01, -2.4904e-01,  1.1075e+00,  2.4934e-01,  2.5328e-01,\n",
      "           -1.0579e+00,  2.0030e+00, -1.2032e+00],\n",
      "          [-7.1591e-02, -6.5778e-01, -3.5105e-01, -1.1052e-01,  6.7449e-02,\n",
      "            6.6726e-01, -1.9203e+00, -1.3025e+00, -1.3260e+00, -1.4193e+00,\n",
      "            1.4013e+00,  8.3926e-01, -5.4663e-01, -1.3245e+00, -1.7242e+00,\n",
      "           -9.3640e-01,  2.8061e+00, -6.2655e-01, -2.1840e-01,  9.4547e-01,\n",
      "           -1.7206e+00,  9.0130e-03,  5.8891e-02, -2.0629e-01, -6.6534e-01,\n",
      "           -4.3895e-01, -1.8048e+00, -4.8520e-01],\n",
      "          [-1.6536e+00, -1.2848e-01,  2.4494e-01,  6.8722e-01, -7.4185e-01,\n",
      "           -2.2903e+00,  6.8193e-01, -5.0557e-02, -1.4613e+00, -1.4644e+00,\n",
      "           -2.3023e-01, -3.9104e-02, -1.2094e+00,  2.5913e-01, -9.4834e-01,\n",
      "           -7.6566e-01, -6.3364e-01, -2.5999e-01, -6.2903e-01, -3.4125e-01,\n",
      "            8.1968e-01, -2.4180e+00,  3.0726e-01,  1.4078e+00, -2.2447e-01,\n",
      "            7.4197e-01,  8.0960e-01,  3.9060e-01],\n",
      "          [ 8.3012e-01,  1.0196e+00, -1.0646e+00,  1.6779e-02,  1.1633e+00,\n",
      "           -3.6825e-01, -1.1816e+00, -1.9950e+00, -4.5440e-01, -7.2449e-01,\n",
      "           -3.5305e-01, -7.8207e-01,  7.4506e-01, -6.8960e-01, -3.4415e-01,\n",
      "            8.1413e-01,  1.9464e-01, -1.3507e+00, -6.1562e-01, -4.2508e-01,\n",
      "            5.1262e-01,  3.0950e-01, -1.4893e-01, -1.6746e-01, -1.3941e+00,\n",
      "            3.4603e-01, -1.4999e+00,  8.7692e-01],\n",
      "          [ 2.6730e+00, -3.1871e-01, -1.7255e-01,  4.6097e-01,  1.3949e-01,\n",
      "           -4.7341e-01, -1.8717e+00, -6.0595e-01, -1.0558e+00, -1.3834e+00,\n",
      "            2.8985e-01, -2.3498e-01, -9.4636e-01, -1.7319e+00,  4.1898e-01,\n",
      "           -5.1522e-01, -4.1529e-01,  3.4616e-01, -7.8583e-01, -4.4246e-01,\n",
      "           -1.8193e+00, -8.4344e-01,  2.8691e-01, -1.5958e-01,  2.1537e+00,\n",
      "           -2.6764e-01,  1.3368e+00, -4.6379e-01],\n",
      "          [ 1.1589e-01,  3.6356e-01, -9.6007e-01, -7.0654e-01, -1.2187e+00,\n",
      "            2.1272e+00, -2.6056e-01,  8.4226e-01,  1.4363e+00,  1.5652e-01,\n",
      "           -1.0149e+00, -3.1761e-01,  2.5292e-01, -9.7334e-01, -1.9218e-01,\n",
      "            8.4414e-01,  5.3936e-01,  3.0619e-01,  7.5289e-01,  6.1369e-01,\n",
      "           -2.6312e-01, -9.8803e-01,  6.5567e-02,  2.6958e-01,  1.2005e+00,\n",
      "           -1.3450e+00, -7.5790e-01,  1.6725e+00],\n",
      "          [-6.8107e-01, -1.3298e+00,  4.0773e-01, -9.4687e-01,  4.7792e-02,\n",
      "            1.4066e+00,  1.0336e+00,  8.1818e-01, -1.1048e+00,  1.9572e+00,\n",
      "            3.1697e-01,  5.6949e-01, -7.7231e-01,  6.6689e-03,  4.8112e-01,\n",
      "            2.3138e+00, -8.8625e-01,  1.8755e+00,  1.8359e-02, -1.7938e+00,\n",
      "            3.9733e-01,  5.7588e-03, -1.2241e+00, -6.9430e-01,  7.4095e-01,\n",
      "            9.8205e-02,  3.1844e-01, -1.6113e+00],\n",
      "          [ 2.9717e-01, -3.7116e-01, -3.4920e-01, -2.2307e+00, -1.2684e+00,\n",
      "            2.1083e+00, -6.3596e-01, -3.4803e-01, -1.5070e+00, -6.7326e-01,\n",
      "           -7.1631e-01,  5.9523e-01,  9.0343e-01,  2.1609e-01, -7.2393e-01,\n",
      "           -3.0256e-01, -1.4410e+00,  8.2626e-01,  2.7617e-03, -7.3377e-01,\n",
      "            6.4015e-01,  6.1363e-01,  1.7881e-01, -4.8176e-01,  9.8327e-01,\n",
      "           -1.1242e+00, -9.5036e-01, -2.7980e-01],\n",
      "          [-1.2086e+00,  3.8974e-01,  6.2332e-01, -7.2377e-01, -4.2316e-01,\n",
      "            2.5572e-02,  1.1366e+00,  3.6628e-01,  7.7160e-01,  1.2252e+00,\n",
      "           -1.8928e-01,  1.3718e+00,  2.5422e-01,  3.1911e-01, -1.8952e+00,\n",
      "            8.1979e-02,  1.1843e+00, -2.1384e+00,  9.6769e-01,  4.0067e-01,\n",
      "           -4.9571e-01,  1.8419e+00,  9.8881e-02,  3.1955e-02,  1.4079e+00,\n",
      "           -1.2278e+00,  9.7219e-01,  1.4912e+00],\n",
      "          [ 2.9806e-02, -2.4350e-01,  9.6183e-01, -1.2599e+00,  1.1953e-03,\n",
      "           -1.3051e+00,  1.2214e-02,  7.4862e-01,  5.2986e-01,  8.1021e-01,\n",
      "           -9.5223e-01,  8.2633e-01,  1.2072e+00, -1.0811e+00, -3.0551e-01,\n",
      "            7.3999e-01, -1.1799e+00,  3.7156e-01,  9.3693e-02,  4.4350e-02,\n",
      "           -2.6588e-01,  5.5481e-01,  6.7852e-01,  1.7764e+00,  9.4330e-02,\n",
      "            2.1036e+00, -1.7722e+00, -5.6089e-01],\n",
      "          [-8.0543e-01, -5.7969e-01, -5.6909e-01,  7.1407e-02,  7.7474e-01,\n",
      "            2.0561e+00, -1.2079e+00,  7.0360e-02, -2.1056e-01,  1.4900e+00,\n",
      "            1.2022e+00,  8.7760e-01, -1.3391e+00,  1.2611e+00, -2.1682e+00,\n",
      "           -9.9624e-01, -4.1068e-01, -2.1770e+00,  2.0983e+00,  7.5726e-01,\n",
      "            3.4024e-01,  9.6754e-01, -2.3241e-01,  5.1713e-01, -1.1167e+00,\n",
      "            8.9516e-01, -1.9637e+00,  6.2338e-01],\n",
      "          [-5.6536e-02, -6.5039e-01, -5.6358e-01, -6.7129e-01,  1.6554e-02,\n",
      "            1.3783e+00, -1.0801e+00, -9.0011e-01, -1.1700e+00,  6.0317e-01,\n",
      "           -1.3937e+00,  2.9654e-01, -3.8473e-01,  3.2210e-01,  4.0369e-01,\n",
      "            5.0467e-01,  1.5812e+00, -5.7636e-01,  5.1923e-01,  1.1619e+00,\n",
      "            9.1394e-02,  7.6342e-01, -4.6766e-01, -5.7460e-01, -1.7146e+00,\n",
      "           -1.4743e+00,  5.7900e-01,  1.0394e-01],\n",
      "          [ 9.1593e-01,  7.9285e-01,  7.4114e-02, -4.3354e-01,  1.2388e-02,\n",
      "            8.7061e-01,  7.9236e-01, -4.2448e-01,  5.3206e-01,  2.7954e+00,\n",
      "           -2.4926e-01,  2.2758e-01, -1.4555e+00, -2.1889e+00,  6.8858e-03,\n",
      "           -6.7834e-01,  2.2748e+00,  3.0577e-02,  5.6482e-01, -3.3278e-01,\n",
      "           -2.0108e+00, -7.7799e-02,  7.9695e-01,  2.1803e-01,  7.0720e-01,\n",
      "           -1.8817e+00, -1.0783e+00,  1.6226e+00],\n",
      "          [ 1.5331e+00, -5.7398e-01,  8.7898e-01, -4.5647e-01,  9.1870e-02,\n",
      "            4.6113e-01, -4.7429e-01,  5.7049e-01,  2.7096e+00,  8.2627e-01,\n",
      "           -5.6167e-01,  1.2198e+00,  1.3640e+00,  7.0680e-01, -7.6800e-01,\n",
      "           -7.4657e-01,  9.0496e-01, -2.4870e+00,  6.8771e-01,  5.5097e-01,\n",
      "           -7.5439e-01,  1.2685e+00, -1.7867e-01,  1.8297e+00, -1.9636e-02,\n",
      "            5.5914e-01, -1.5266e+00,  8.9860e-01],\n",
      "          [ 2.1150e+00,  2.4814e-01, -5.9964e-01, -4.2823e-01,  9.8168e-01,\n",
      "            5.4669e-03,  8.5826e-01,  4.0257e-01,  1.7765e+00, -9.2754e-03,\n",
      "           -2.1879e-01,  1.6948e-01, -1.1203e+00,  4.7922e-01, -2.6890e+00,\n",
      "           -7.4325e-01,  2.8458e+00,  1.9557e-01, -4.9953e-01, -3.0493e-01,\n",
      "           -3.1943e-01, -7.8451e-02, -5.1703e-01, -5.4230e-01,  1.3603e-01,\n",
      "           -9.5225e-02, -3.5158e-01, -2.5543e-02]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "example_latent = DDPM_obj.sample_latent((1, 1, 28, 28))\n",
    "inferenced_image = DDPM_obj.inference(example_latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
